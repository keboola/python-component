<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>keboola.component.interface API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>keboola.component.interface</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">import argparse
import csv
import glob
import json
import logging
import os
import sys
from datetime import datetime
from pathlib import Path
from typing import List, Dict, Optional, Union

from deprecated import deprecated
from pygelf import GelfUdpHandler, GelfTcpHandler
from pytz import utc

from . import dao
from .exceptions import UserException


def register_csv_dialect():
    &#34;&#34;&#34;
    Register the KBC CSV dialect
    &#34;&#34;&#34;
    csv.register_dialect(&#39;kbc&#39;, lineterminator=&#39;\n&#39;, delimiter=&#39;,&#39;,
                         quotechar=&#39;&#34;&#39;)


def init_environment_variables() -&gt; dao.EnvironmentVariables:
    &#34;&#34;&#34;
    Initializes environment variables available in the docker environment
        https://developers.keboola.com/extend/common-interface/environment/#environment-variables

    Returns:
        dao.EnvironmentVariables:
    &#34;&#34;&#34;
    return dao.EnvironmentVariables(data_dir=os.environ.get(&#39;KBC_DATADIR&#39;, None),
                                    run_id=os.environ.get(&#39;KBC_RUNID&#39;, None),
                                    project_id=os.environ.get(&#39;KBC_PROJECTID&#39;, None),
                                    stack_id=os.environ.get(&#39;KBC_STACKID&#39;, None),
                                    config_id=os.environ.get(&#39;KBC_CONFIGID&#39;, None),
                                    component_id=os.environ.get(&#39;KBC_COMPONENTID&#39;, None),
                                    config_row_id=os.environ.get(&#39;KBC_CONFIGROWID&#39;, None),
                                    branch_id=os.environ.get(&#39;KBC_BRANCHID&#39;, None),
                                    staging_file_provider=os.environ.get(&#39;KBC_STAGING_FILE_PROVIDER&#39;, None),
                                    project_name=os.environ.get(&#39;KBC_PROJECTNAME&#39;, None),
                                    token_id=os.environ.get(&#39;KBC_TOKENID&#39;, None),
                                    token_desc=os.environ.get(&#39;KBC_TOKENDESC&#39;, None),
                                    token=os.environ.get(&#39;KBC_TOKEN&#39;, None),
                                    url=os.environ.get(&#39;KBC_URL&#39;, None),
                                    real_user=os.environ.get(&#39;KBC_REALUSER&#39;, None),
                                    logger_addr=os.environ.get(&#39;KBC_LOGGER_ADDR&#39;, None),
                                    logger_port=os.environ.get(&#39;KBC_LOGGER_PORT&#39;, None)
                                    )


class CommonInterface:
    &#34;&#34;&#34;
    A class handling standard tasks related to the
    [Keboola Common Interface](https://developers.keboola.com/extend/common-interface/)
    e.g. config load, validation, component state, I/O handling, I/O metadata and manifest files.

    It initializes the environment inject into the Docker container the KBC component runs in and abstracts the tasks
    related to the Common Interface interaction.

    Attributes:
        data_folder_path (str):
            Full path to the /data folder

    &#34;&#34;&#34;
    LOGGING_TYPE_STD = &#39;std&#39;
    LOGGING_TYPE_GELF = &#39;gelf&#39;

    def __init__(self, data_folder_path: str = None, log_level=logging.INFO, logging_type=None):
        &#34;&#34;&#34;
        Initializes the CommonInterface environment. If the data_folder_path is not specified the folder
        is established in following order:

        - From provided argument if present: `-d` or `--data`
        - From environment variable if present (KBC_DATADIR)
        - Defaults to /data/ if none of the above is specified

        Args:
            data_folder_path (str): path to a data folder.
            log_level (int): logging.INFO or logging.DEBUG
            logging_type (str): optional &#39;std&#39; or &#39;gelf&#39;, if left empty determined automatically
        &#34;&#34;&#34;
        self.environment_variables = init_environment_variables()
        register_csv_dialect()

        # init logging
        logging_type_inf = CommonInterface.LOGGING_TYPE_GELF if os.getenv(&#39;KBC_LOGGER_ADDR&#39;,
                                                                          None) else CommonInterface.LOGGING_TYPE_STD
        if not logging_type:
            logging_type = logging_type_inf

        if logging_type == CommonInterface.LOGGING_TYPE_STD:
            self.set_default_logger(log_level)
        elif logging_type == CommonInterface.LOGGING_TYPE_GELF:
            self.set_gelf_logger(log_level)

        # init data folder
        if not data_folder_path:
            data_folder_path = self._get_data_folder_from_context()

        # validate
        if not os.path.exists(data_folder_path) and not os.path.isdir(data_folder_path):
            raise ValueError(
                f&#34;The data directory does not exist, verify that the data directory is correct. Dir: &#34;
                f&#34;{data_folder_path}&#34;
            )

        self.data_folder_path = data_folder_path

    def _get_data_folder_from_context(self):
        # try to get from argument parameter

        # get from parameters
        argparser = argparse.ArgumentParser()
        argparser.add_argument(
            &#39;-d&#39;,
            &#39;--data&#39;,
            dest=&#39;data_dir&#39;,
            default=&#39;&#39;,
            help=&#39;Data directory&#39;
        )
        # unknown is to ignore extra arguments
        args, unknown = argparser.parse_known_args()
        data_folder_path = args.data_dir

        if data_folder_path == &#39;&#39; and self.environment_variables.data_dir:
            data_folder_path = self.environment_variables.data_dir
        elif data_folder_path == &#39;&#39;:
            data_folder_path = &#39;/data/&#39;

        return data_folder_path

    # ================================= Logging ==============================
    @staticmethod
    def set_default_logger(log_level: int = logging.INFO):  # noqa: E301
        &#34;&#34;&#34;
        Sets default console logger.

        Args:
            log_level: logging level, default: &#39;logging.INFO&#39;

        Returns:
            Logger object

        &#34;&#34;&#34;

        class InfoFilter(logging.Filter):
            def filter(self, rec):
                return rec.levelno in (logging.DEBUG, logging.INFO)

        hd1 = logging.StreamHandler(sys.stdout)
        hd1.addFilter(InfoFilter())
        hd2 = logging.StreamHandler(sys.stderr)
        hd2.setLevel(logging.WARNING)

        logging.getLogger().setLevel(log_level)
        # remove default handler
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        logging.getLogger().addHandler(hd1)
        logging.getLogger().addHandler(hd2)

        logger = logging.getLogger()
        return logger

    @staticmethod
    def set_gelf_logger(log_level: int = logging.INFO, transport_layer=&#39;TCP&#39;,
                        stdout=False, include_extra_fields=True, **gelf_kwargs):  # noqa: E301
        &#34;&#34;&#34;
        Sets gelf console logger. Handler for console output is not included by default,
        for testing in non-gelf environments use stdout=True.

        Args:
            log_level: logging level, default: &#39;logging.INFO&#39;
            transport_layer: &#39;TCP&#39; or &#39;UDP&#39;, default:&#39;UDP
            stdout: if set to True, Stout handler is also included
            include_extra_fields:
                Include extra GELF fields in the log messages.
                e.g. logging.warning(&#39;Some warning&#39;,
                                     extra={&#34;additional_info&#34;: &#34;Extra info to be displayed in the detail&#34;}

        Returns: Logger object
        &#34;&#34;&#34;
        # remove existing handlers
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        if stdout:
            CommonInterface.set_default_logger(log_level)

        # gelf handler setup
        gelf_kwargs[&#39;include_extra_fields&#39;] = include_extra_fields

        host = os.getenv(&#39;KBC_LOGGER_ADDR&#39;, &#39;localhost&#39;)
        port = os.getenv(&#39;KBC_LOGGER_PORT&#39;, 12201)
        if transport_layer == &#39;TCP&#39;:
            gelf = GelfTcpHandler(host=host, port=port, **gelf_kwargs)
        elif transport_layer == &#39;UDP&#39;:
            gelf = GelfUdpHandler(host=host, port=port, **gelf_kwargs)
        else:
            raise ValueError(F&#39;Unsupported gelf transport layer: {transport_layer}. Choose TCP or UDP&#39;)

        logging.getLogger().setLevel(log_level)
        logging.getLogger().addHandler(gelf)

        logger = logging.getLogger()
        return logger

    def get_state_file(self) -&gt; dict:
        &#34;&#34;&#34;

        Returns dict representation of state file or empty dict if not present

        Returns:
            dict:

        &#34;&#34;&#34;
        logging.info(&#39;Loading state file..&#39;)
        state_file_path = os.path.join(self.data_folder_path, &#39;in&#39;, &#39;state.json&#39;)
        if not os.path.isfile(state_file_path):
            logging.info(&#39;State file not found. First run?&#39;)
            return {}
        try:
            with open(state_file_path, &#39;r&#39;) \
                    as state_file:
                return json.load(state_file)
        except (OSError, IOError):
            raise ValueError(
                &#34;State file state.json unable to read &#34;
            )

    def write_state_file(self, state_dict: dict):
        &#34;&#34;&#34;
        Stores [state file](https://developers.keboola.com/extend/common-interface/config-file/#state-file).
        Args:
            state_dict (dict):
        &#34;&#34;&#34;
        if not isinstance(state_dict, dict):
            raise TypeError(&#39;Dictionary expected as a state file datatype!&#39;)

        with open(os.path.join(self.configuration.data_dir, &#39;out&#39;, &#39;state.json&#39;), &#39;w+&#39;) as state_file:
            json.dump(state_dict, state_file)

    def get_input_table_definition_by_name(self, table_name: str) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
        Return dao.TableDefinition object by table name.

        If nor the table itself or it&#39;s manifest exists, a ValueError is thrown.

        The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
        provides methods for updating the manifest metadata.

        Args:
            table_name: Destination table name (name of .csv file). e.g. input.csv

        Returns:
            dao.TableDefinition
        &#34;&#34;&#34;
        manifest_path = os.path.join(
            self.tables_in_path,
            table_name + &#39;.manifest&#39;
        )

        return dao.TableDefinition.build_from_manifest(manifest_path)

    def get_input_tables_definitions(self, orphaned_manifests=False) -&gt; List[dao.TableDefinition]:
        &#34;&#34;&#34;
        Return dao.TableDefinition objects by scanning the `data/in/tables` folder.

        The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
        provides methods for updating the manifest metadata.

        By default, orphaned manifests are skipped.


        See Also: keboola.component.dao.dao.TableDefinition

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. This is useful in
            in scenarios where [workspaces exchange](
            https://developers.keboola.com/extend/common-interface/folders/#exchanging-data-via-workspace) is used
            e.g. when only manifest files are present in the `data/in/tables` folder.

        Returns: List[dao.TableDefinition]

        &#34;&#34;&#34;

        table_files = [f for f in glob.glob(self.tables_in_path + &#34;/**&#34;, recursive=False) if
                       not f.endswith(&#39;.manifest&#39;)]
        table_defs = list()
        for t in table_files:
            p = Path(t)
            manifest_path = t + &#39;.manifest&#39;

            if p.is_dir() and not Path(manifest_path).exists():
                # skip folders that do not have matching manifest
                logging.warning(f&#39;Folder {t} does not have matching manifest, it will be ignored!&#39;)
                continue

            table_defs.append(dao.TableDefinition.build_from_manifest(manifest_path))

        if orphaned_manifests:
            files_w_manifest = [t.name + &#39;.manifest&#39; for t in table_defs]
            manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                              if Path(f).name not in files_w_manifest]
            for t in manifest_files:
                p = Path(t)

                if p.is_dir():
                    # skip folders that do not have matching manifest
                    logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                    continue

                table_defs.append(dao.TableDefinition.build_from_manifest(t))
        return table_defs

    def _create_table_definition(self, name: str,
                                 storage_stage: str = &#39;out&#39;,
                                 is_sliced: bool = False,
                                 destination: str = &#39;&#39;,
                                 primary_key: List[str] = None,
                                 columns: List[str] = None,
                                 incremental: bool = None,
                                 table_metadata: dao.TableMetadata = None,
                                 enclosure: str = &#39;&#34;&#39;,
                                 delimiter: str = &#39;,&#39;,
                                 delete_where: dict = None,
                                 write_always: bool = False) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                Helper method for dao.TableDefinition creation along with the &#34;manifest&#34;.
                It initializes path according to the storage_stage type.

                Args:
                    name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                    storage_stage:
                        default value: &#39;out&#39;
                        either `&#39;in&#39;` or `&#39;out&#39;`. Determines the path to result file.
                        E.g. `data/tables/in/my_table.csv`
                    is_sliced: True if the full_path points to a folder with sliced tables
                    destination: String name of the table in Storage.
                    primary_key: List with names of columns used for primary key.
                    columns: List of columns for headless CSV files
                    incremental: Set to true to enable incremental loading
                    table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                    enclosure: str: CSV enclosure, by default &#34;
                    delimiter: str: CSV delimiter, by default ,
                    delete_where: Dict with settings for deleting rows
                    write_always: Bool: If true, the table will be saved to Storage even when the job execution
                           fails.
        &#34;&#34;&#34;
        if storage_stage == &#39;in&#39;:
            full_path = os.path.join(self.tables_in_path, name)
        elif storage_stage == &#39;out&#39;:
            full_path = os.path.join(self.tables_out_path, name)
        else:
            raise ValueError(f&#39;Invalid storage_stage value &#34;{storage_stage}&#34;. Supported values are: &#34;in&#34; or &#34;out&#34;!&#39;)

        return dao.TableDefinition(name=name,
                                   full_path=full_path,
                                   is_sliced=is_sliced,
                                   destination=destination,
                                   primary_key=primary_key,
                                   columns=columns,
                                   incremental=incremental,
                                   table_metadata=table_metadata,
                                   enclosure=enclosure,
                                   delimiter=delimiter,
                                   delete_where=delete_where,
                                   stage=storage_stage,
                                   write_always=write_always)

    def create_in_table_definition(self, name: str,
                                   is_sliced: bool = False,
                                   destination: str = &#39;&#39;,
                                   primary_key: List[str] = None,
                                   columns: List[str] = None,
                                   incremental: bool = None,
                                   table_metadata: dao.TableMetadata = None,
                                   delete_where: str = None) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                       Helper method for input dao.TableDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/tables/in/ folder.

                       Args:
                           name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                           is_sliced: True if the full_path points to a folder with sliced tables
                           destination: String name of the table in Storage.
                           primary_key: List with names of columns used for primary key.
                           columns: List of columns for headless CSV files
                           incremental: Set to true to enable incremental loading
                           table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                           delete_where: Dict with settings for deleting rows
        &#34;&#34;&#34;

        return self._create_table_definition(name=name,
                                             storage_stage=&#39;in&#39;,
                                             is_sliced=is_sliced,
                                             destination=destination,
                                             primary_key=primary_key,
                                             columns=columns,
                                             incremental=incremental,
                                             table_metadata=table_metadata,
                                             delete_where=delete_where)

    def create_out_table_definition(self, name: str,
                                    is_sliced: bool = False,
                                    destination: str = &#39;&#39;,
                                    primary_key: List[str] = None,
                                    columns: List[str] = None,
                                    incremental: bool = None,
                                    table_metadata: dao.TableMetadata = None,
                                    enclosure: str = &#39;&#34;&#39;,
                                    delimiter: str = &#39;,&#39;,
                                    delete_where: dict = None,
                                    write_always: bool = False) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                       Helper method for output dao.TableDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/tables/out/ folder.

                       Args:
                           name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                           is_sliced: True if the full_path points to a folder with sliced tables
                           destination: String name of the table in Storage.
                           primary_key: List with names of columns used for primary key.
                           columns: List of columns for headless CSV files
                           incremental: Set to true to enable incremental loading
                           table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                           enclosure: str: CSV enclosure, by default &#34;
                           delimiter: str: CSV delimiter, by default ,
                           delete_where: Dict with settings for deleting rows
                           write_always: Bool: If true, the table will be saved to Storage even when the job execution
                           fails.
        &#34;&#34;&#34;

        return self._create_table_definition(name=name,
                                             storage_stage=&#39;out&#39;,
                                             is_sliced=is_sliced,
                                             destination=destination,
                                             primary_key=primary_key,
                                             columns=columns,
                                             incremental=incremental,
                                             table_metadata=table_metadata,
                                             enclosure=enclosure,
                                             delimiter=delimiter,
                                             delete_where=delete_where,
                                             write_always=write_always)

    # # File processing

    def get_input_file_definitions_grouped_by_tag_group(self, orphaned_manifests=False,
                                                        only_latest_files=True,
                                                        tags: List[str] = None,
                                                        include_system_tags=False) \
            -&gt; Dict[str, List[dao.FileDefinition]]:
        &#34;&#34;&#34;
        Convenience method returning lists of files in dictionary grouped by tag group.

        (tag group is string built from alphabetically ordered and concatenated tags, e.g. &#39;tag1;tag2&#39;

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                    a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files containing specified tags will be fetched.
            include_system_tags (bool): optional flag that will use system generated tags in groups as well.
                                   See FileDefinition.SYSTEM_TAG_PREFIXES

        Returns:
            Dict[str,List[dao.FileDefinition]] indexed by tag group =&gt; string built from alphabetically ordered
            and concatenated tags, e.g. `tag1;tag2`

        &#34;&#34;&#34;
        file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
        return self.__group_file_defs_by_tag_group(file_definitions, include_system_tags=include_system_tags)

    def get_input_file_definitions_grouped_by_name(self, orphaned_manifests=False, only_latest_files=True,
                                                   tags: List[str] = None) -&gt; Dict[str, List[dao.FileDefinition]]:
        &#34;&#34;&#34;
        Convenience method returning lists of files in dictionary grouped by file name.

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                    a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

        Returns:

        &#34;&#34;&#34;
        file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
        return self.__group_files_by_name(file_definitions)

    def __group_file_defs_by_tag_group(self, file_definitions: List[dao.FileDefinition], include_system_tags=False) \
            -&gt; Dict[str, List[dao.FileDefinition]]:

        files_per_tag: dict = {}
        for f in file_definitions:

            tag_group_v1 = f.tags if include_system_tags else f.user_tags
            tag_group_v1.sort()
            tag_group_key = &#39;;&#39;.join(tag_group_v1)
            if not files_per_tag.get(tag_group_key):
                files_per_tag[tag_group_key] = []
            files_per_tag[tag_group_key].append(f)
        return files_per_tag

    def _filter_files(self, file_definitions: List[dao.FileDefinition], tags: List[str] = None,
                      only_latest: bool = True) -&gt; List[dao.FileDefinition]:

        filtered_files = file_definitions

        if only_latest:
            filtered_files = self.__filter_filedefs_by_latest(filtered_files)

        # filter by tags
        if tags:
            new_filtered = []
            filter_set = set(tags)
            for fd in filtered_files:
                tags_set = set(fd.tags)
                if filter_set.issubset(tags_set):
                    new_filtered.append(fd)
            filtered_files = new_filtered

        return filtered_files

    def __group_files_by_name(self, file_definitions: List[dao.FileDefinition]) -&gt; Dict[str, List[dao.FileDefinition]]:
        files_per_name: dict = {}
        for f in file_definitions:
            if not files_per_name.get(f.name):
                files_per_name[f.name] = []
            files_per_name[f.name].append(f)
        return files_per_name

    def __filter_filedefs_by_latest(self, file_definitions: List[dao.FileDefinition]) -&gt; List[dao.FileDefinition]:
        &#34;&#34;&#34;
        Get latest file (according to the timestamp) by each filename
        Args:
            file_definitions:

        Returns:

        &#34;&#34;&#34;
        filtered_files = list()
        files_per_name = self.__group_files_by_name(file_definitions)
        for group in files_per_name:
            max_file = None
            max_timestamp = utc.localize(datetime(1900, 5, 17))
            for f in files_per_name[group]:
                creation_date = f.created
                # if date not present ignore and add anyway
                if creation_date is None or creation_date &gt; max_timestamp:
                    max_timestamp = creation_date
                    max_file = f
            filtered_files.append(max_file)
        return filtered_files

    def get_input_files_definitions(self, orphaned_manifests=False,
                                    only_latest_files=True,
                                    tags: Optional[List[str]] = None) -&gt; List[dao.FileDefinition]:
        &#34;&#34;&#34;
        Return dao.FileDefinition objects by scanning the `data/in/files` folder.

        The dao.FileDefinition will contain full path of the source file, it&#39;s name and manifest.

        By default only latest versions of each file are included.

        By default, orphaned manifests are skipped, otherwise fails with ValueError.

        A filter may be specified to match only some tags. All files containing specified tags will be returned.


        See Also: keboola.component.dao.FileDefinition

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
            a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

        Returns: List[dao.TableDefinition]

        &#34;&#34;&#34;

        in_files = [f for f in glob.glob(self.files_in_path + &#34;/**&#34;, recursive=False) if
                    not f.endswith(&#39;.manifest&#39;)]
        file_defs = list()
        for t in in_files:
            manifest_path = t + &#39;.manifest&#39;

            file_defs.append(dao.FileDefinition.build_from_manifest(manifest_path))

        if orphaned_manifests:
            files_w_manifest = [t.full_path for t in file_defs]
            manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                              if Path(f).name not in files_w_manifest]
            for t in manifest_files:
                p = Path(t)

                if p.is_dir():
                    # skip folders that do not have matching manifest
                    logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                    continue

                file_defs.append(dao.FileDefinition.build_from_manifest(t))

        return self._filter_files(file_defs, tags, only_latest_files)

    def _create_file_definition(self,
                                name: str,
                                storage_stage: str = &#39;out&#39;,
                                tags: List[str] = None,
                                is_public: bool = False,
                                is_permanent: bool = False,
                                is_encrypted: bool = False,
                                notify: bool = False) -&gt; dao.FileDefinition:
        &#34;&#34;&#34;
                Helper method for dao.FileDefinition creation along with the &#34;manifest&#34;.
                It initializes path according to the storage_stage type.

                Args:
                                name (str): Name of the file, e.g. file.jpg.
                                tags (list):
                                    List of tags that are assigned to this file
                                is_public: When true, the file URL will be permanent and publicly accessible.
                                is_permanent: Keeps a file forever. If false, the file will be deleted after default
                                period of time (e.g.
                                15 days)
                                is_encrypted: If true, the file content will be encrypted in the storage.
                                notify: Notifies project administrators that a file was uploaded.
        &#34;&#34;&#34;
        if storage_stage == &#39;in&#39;:
            full_path = os.path.join(self.files_in_path, name)
        elif storage_stage == &#39;out&#39;:
            full_path = os.path.join(self.files_out_path, name)
        else:
            raise ValueError(f&#39;Invalid storage_stage value &#34;{storage_stage}&#34;. Supported values are: &#34;in&#34; or &#34;out&#34;!&#39;)

        return dao.FileDefinition(
            full_path=full_path,
            tags=tags,
            is_public=is_public,
            is_permanent=is_permanent,
            is_encrypted=is_encrypted,
            notify=notify)

    def create_out_file_definition(self, name: str,
                                   tags: List[str] = None,
                                   is_public: bool = False,
                                   is_permanent: bool = False,
                                   is_encrypted: bool = False,
                                   notify: bool = False) -&gt; dao.FileDefinition:
        &#34;&#34;&#34;
                       Helper method for input dao.FileDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/files/out/ folder.

                       Args:
                                name (str): Name of the file, e.g. file.jpg.
                                tags (list):
                                    List of tags that are assigned to this file
                                is_public: When true, the file URL will be permanent and publicly accessible.
                                is_permanent: Keeps a file forever. If false, the file will be deleted after default
                                period of time (e.g.
                                15 days)
                                is_encrypted: If true, the file content will be encrypted in the storage.
                                notify: Notifies project administrators that a file was uploaded.
        &#34;&#34;&#34;

        return self._create_file_definition(name=name,
                                            storage_stage=&#39;out&#39;,
                                            tags=tags,
                                            is_public=is_public,
                                            is_permanent=is_permanent,
                                            is_encrypted=is_encrypted,
                                            notify=notify)

    # TODO: refactor the validate config so it&#39;s more userfriendly
    &#34;&#34;&#34;
        - Support for nested params?
    &#34;&#34;&#34;

    def validate_configuration_parameters(self, mandatory_params=None):
        &#34;&#34;&#34;
                Validates config parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                =&gt; at least one of the pars has to be present
                ex2.
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                &#34;&#34;&#34;
        if not mandatory_params:
            mandatory_params = []
        return self._validate_parameters(self.configuration.parameters, mandatory_params, &#39;config parameters&#39;)

    def validate_image_parameters(self, mandatory_params):
        &#34;&#34;&#34;
                Validates image parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                =&gt; at least one of the pars has to be present
                ex2.
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                &#34;&#34;&#34;
        return self._validate_parameters(self.configuration.image_parameters,
                                         mandatory_params, &#39;image/stack parameters&#39;)

    def _validate_parameters(self, parameters, mandatory_params, _type):
        &#34;&#34;&#34;
        Validates provided parameters based on provided mandatory parameters.
        All provided parameters must be present in config to pass.
        ex1.:
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        mandatory_params = [par1, par2]
        Validation will fail when one of the above parameters is not found

        Two levels of nesting:
        Parameters can be grouped as arrays par3 = [groupPar1, groupPar2] =&gt; at least one of the pars has to be
        present
        ex2.
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        par3 = &#39;par3&#39;
        groupPar1 = &#39;groupPar1&#39;
        groupPar2 = &#39;groupPar2&#39;
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group1]

        Folowing logical expression is evaluated:
        Par1 AND Par2 AND (groupPar1 OR groupPar2)

        ex3
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        par3 = &#39;par3&#39;
        groupPar1 = &#39;groupPar1&#39;
        groupPar2 = &#39;groupPar2&#39;
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group3]

        Following logical expression is evaluated:
        par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))

        Raises:
            UserException: on missing parameters
        &#34;&#34;&#34;
        missing_fields = []
        for par in mandatory_params:
            if isinstance(par, list):
                missing_fields.extend(self._validate_par_group(par, parameters))
            elif parameters.get(par) is None:
                missing_fields.append(par)

        if missing_fields:
            raise UserException(
                &#39;Missing mandatory {} fields: [{}] &#39;.format(_type, &#39;, &#39;.join(missing_fields)))

    def _validate_par_group(self, par_group, parameters):
        missing_fields = []
        is_present = False
        for par in par_group:
            if isinstance(par, list):
                missing_subset = self._get_par_missing_fields(par, parameters)
                missing_fields.extend(missing_subset)
                if not missing_subset:
                    is_present = True

            elif parameters.get(par):
                is_present = True
            else:
                missing_fields.append(par)
        if not is_present:
            return missing_fields
        else:
            return []

    def _get_par_missing_fields(self, mand_params, parameters):
        missing_fields = []
        for par in mand_params:
            if not parameters.get(par):
                missing_fields.append(par)
        return missing_fields

    # ### PROPERTIES
    @property
    def configuration(self):
        # try to load the configuration
        # raises ValueError

        return Configuration(self.data_folder_path)

    @property
    def tables_out_path(self):
        return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;tables&#39;)

    @property
    def tables_in_path(self):
        return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;tables&#39;)

    @property
    def files_out_path(self):
        return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;files&#39;)

    @property
    def files_in_path(self):
        return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;files&#39;)

    @property
    def is_legacy_queue(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the project is running on legacy queue (v1)
        Returns:

        &#34;&#34;&#34;
        features = os.environ.get(&#39;KBC_PROJECT_FEATURE_GATES&#39;)
        is_legacy_queue = True
        if not features or &#39;queuev2&#39; in features:
            is_legacy_queue = False
        return is_legacy_queue

    def write_manifest(self, io_definition: Union[dao.FileDefinition, dao.TableDefinition]):
        &#34;&#34;&#34;
        Write a table manifest from dao.IODefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()

        # build table definition
        table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                                , incremental = True
                                , table_metadata = tm
                                ))
        ci.write_manifest(table_def)

        # build file definition
        file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
        ci.write_manifest(file_def)
        ```

        Args:
            io_definition Union[dao.FileDefinition, dao.TableDefinition]: Initialized dao.IODefinition
             object containing manifest.

        Returns:

        &#34;&#34;&#34;

        manifest = io_definition.get_manifest_dictionary(legacy_queue=self.is_legacy_queue)
        # make dirs if not exist
        os.makedirs(os.path.dirname(io_definition.full_path), exist_ok=True)
        with open(io_definition.full_path + &#39;.manifest&#39;, &#39;w&#39;) as manifest_file:
            json.dump(manifest, manifest_file)

    def write_manifests(self, io_definitions: List[Union[dao.FileDefinition, dao.TableDefinition]]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            io_definitions:

        Returns:

        &#34;&#34;&#34;
        for io_def in io_definitions:
            self.write_manifest(io_def)

    # ############# DEPRECATED METHODS, TODO: remove

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
    def write_filedef_manifest(self, file_definition: dao.FileDefinition):
        &#34;&#34;&#34;
        Write a table manifest from dao.FileDefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()

        # build table definition
        file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
        ci.write_filedef_manifest(file_def)
        ```

        Args:
            file_definition (dao.FileDefinition): Initialized dao.FileDefinition object containing manifest.

        Returns:

        &#34;&#34;&#34;
        self.write_manifest(file_definition)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
    def write_filedef_manifests(self, file_definitions: List[dao.FileDefinition]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            file_definitions:

        Returns:

        &#34;&#34;&#34;
        self.write_manifests(file_definitions)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
    def write_tabledef_manifest(self, table_definition: dao.TableDefinition):
        &#34;&#34;&#34;
        Write a table manifest from dao.TableDefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()
        tm = dao.TableMetadata()
        tm.add_table_description(&#34;My new table&#34;)

        # build table definition
        table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                                , incremental = True
                                , table_metadata = tm
                                ))
        ci.write_tabledef_manifest(table_def)
        ```

        Args:
            table_definition (dao.TableDefinition): Initialized dao.TableDefinition object containing manifest.

        Returns:

        &#34;&#34;&#34;
        self.write_manifest(table_definition)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
    def write_tabledef_manifests(self, table_definitions: List[dao.TableDefinition]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            table_definitions:

        Returns:

        &#34;&#34;&#34;
        self.write_manifests(table_definitions)


# ########## CONFIGURATION

class Configuration:
    &#34;&#34;&#34;
    Class representing configuration file generated and read
    by KBC for docker applications
    See docs:
    https://developers.keboola.com/extend/common-interface/config-file/
    &#34;&#34;&#34;

    def __init__(self, data_folder_path: str):
        &#34;&#34;&#34;

        Args:
            data_folder_path (object):
        &#34;&#34;&#34;
        self.config_data = {}
        self.data_dir = data_folder_path

        try:
            with open(os.path.join(data_folder_path, &#39;config.json&#39;), &#39;r&#39;) \
                    as config_file:
                self.config_data = json.load(config_file)
        except (OSError, IOError):
            raise ValueError(
                f&#34;Configuration file config.json not found, verify that the data directory is correct and that the &#34;
                f&#34;config file is present. Dir: &#34;
                f&#34;{self.data_dir}&#34;
            )

        self.parameters = self.config_data.get(&#39;parameters&#39;, {})
        self.image_parameters = self.config_data.get(&#39;image_parameters&#39;, {})
        self.action = self.config_data.get(&#39;action&#39;, &#39;&#39;)
        self.workspace_credentials = self.config_data.get(&#39;authorization&#39;, {}).get(&#39;workspace&#39;, {})

    # ################ PROPERTIES
    @property
    def oauth_credentials(self) -&gt; dao.OauthCredentials:
        &#34;&#34;&#34;
        Returns subscriptable class OauthCredentials

        Returns: OauthCredentials

        &#34;&#34;&#34;
        oauth_credentials = self.config_data.get(&#39;authorization&#39;, {}).get(&#39;oauth_api&#39;, {}).get(&#39;credentials&#39;, {})
        credentials = None
        if oauth_credentials:
            credentials = dao.OauthCredentials(
                id=oauth_credentials.get(&#34;id&#34;, &#39;&#39;),
                created=oauth_credentials.get(&#34;created&#34;, &#39;&#39;),
                data=json.loads(oauth_credentials.get(&#34;#data&#34;, &#39;{}&#39;)),
                oauthVersion=oauth_credentials.get(&#34;oauthVersion&#34;, &#39;&#39;),
                appKey=oauth_credentials.get(&#34;appKey&#34;, &#39;&#39;),
                appSecret=oauth_credentials.get(&#34;#appSecret&#34;, &#39;&#39;)
            )
        return credentials

    @property
    def tables_input_mapping(self) -&gt; List[dao.TableInputMapping]:
        &#34;&#34;&#34;
        List of table [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

        Tables specified in the configuration file.

        Returns: List[TableInputMapping]

        &#34;&#34;&#34;

        tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;input&#39;, {}).get(&#39;tables&#39;, [])
        tables = []
        for table in tables_defs:
            # nested dataclass
            table[&#39;column_types&#39;] = [dao.build_dataclass_from_dict(dao.TableColumnTypes, coltype) for coltype in
                                     table.get(&#39;column_types&#39;, [])]

            im = dao.build_dataclass_from_dict(dao.TableInputMapping, table)
            im.full_path = os.path.normpath(
                os.path.join(
                    self.data_dir,
                    &#39;in&#39;,
                    &#39;tables&#39;,
                    table[&#39;destination&#39;]
                )
            )
            tables.append(im)
        return tables

    @property
    def tables_output_mapping(self) -&gt; List[dao.TableOutputMapping]:
        &#34;&#34;&#34;
        List of table [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

        Get tables which are supposed to be returned when the application finishes. (from configuration[
        &#39;storage&#39;] section.
        Returns: List[TableOutputMapping]

        &#34;&#34;&#34;
        tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;tables&#39;, [])
        tables = []
        for table in tables_defs:
            om = dao.build_dataclass_from_dict(dao.TableOutputMapping, table)
            tables.append(om)
        return tables

    @property
    def files_input_mapping(self) -&gt; List[dao.FileInputMapping]:
        &#34;&#34;&#34;
        List of file [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

        Files specified in the configuration file (defined on component&#39;s input mapping). (from configuration[
        &#39;storage&#39;] section.
        Returns: List[FileInputMapping]

        &#34;&#34;&#34;
        defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
        files = []
        for file in defs:
            om = dao.build_dataclass_from_dict(dao.FileInputMapping, file)
            files.append(om)
        return files

    @property
    def files_output_mapping(self) -&gt; List[dao.FileOutputMapping]:
        &#34;&#34;&#34;
        List of file [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

        Get files which are supposed to be returned when the application finishes. (from configuration[
        &#39;storage&#39;] section.
        Returns:

        &#34;&#34;&#34;
        defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
        files = []
        for file in defs:
            om = dao.build_dataclass_from_dict(dao.FileOutputMapping, file)
            files.append(om)
        return files</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-functions">Functions</h2>
<dl>
<dt id="keboola.component.interface.init_environment_variables"><code class="name flex">
<span>def <span class="ident">init_environment_variables</span></span>(<span>) ><a title="keboola.component.dao.EnvironmentVariables" href="dao.html#keboola.component.dao.EnvironmentVariables">EnvironmentVariables</a></span>
</code></dt>
<dd>
<div class="desc"><p>Initializes environment variables available in the docker environment
<a href="https://developers.keboola.com/extend/common-interface/environment/#environment-variables">https://developers.keboola.com/extend/common-interface/environment/#environment-variables</a></p>
<h2 id="returns">Returns</h2>
<p>dao.EnvironmentVariables:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def init_environment_variables() -&gt; dao.EnvironmentVariables:
    &#34;&#34;&#34;
    Initializes environment variables available in the docker environment
        https://developers.keboola.com/extend/common-interface/environment/#environment-variables

    Returns:
        dao.EnvironmentVariables:
    &#34;&#34;&#34;
    return dao.EnvironmentVariables(data_dir=os.environ.get(&#39;KBC_DATADIR&#39;, None),
                                    run_id=os.environ.get(&#39;KBC_RUNID&#39;, None),
                                    project_id=os.environ.get(&#39;KBC_PROJECTID&#39;, None),
                                    stack_id=os.environ.get(&#39;KBC_STACKID&#39;, None),
                                    config_id=os.environ.get(&#39;KBC_CONFIGID&#39;, None),
                                    component_id=os.environ.get(&#39;KBC_COMPONENTID&#39;, None),
                                    config_row_id=os.environ.get(&#39;KBC_CONFIGROWID&#39;, None),
                                    branch_id=os.environ.get(&#39;KBC_BRANCHID&#39;, None),
                                    staging_file_provider=os.environ.get(&#39;KBC_STAGING_FILE_PROVIDER&#39;, None),
                                    project_name=os.environ.get(&#39;KBC_PROJECTNAME&#39;, None),
                                    token_id=os.environ.get(&#39;KBC_TOKENID&#39;, None),
                                    token_desc=os.environ.get(&#39;KBC_TOKENDESC&#39;, None),
                                    token=os.environ.get(&#39;KBC_TOKEN&#39;, None),
                                    url=os.environ.get(&#39;KBC_URL&#39;, None),
                                    real_user=os.environ.get(&#39;KBC_REALUSER&#39;, None),
                                    logger_addr=os.environ.get(&#39;KBC_LOGGER_ADDR&#39;, None),
                                    logger_port=os.environ.get(&#39;KBC_LOGGER_PORT&#39;, None)
                                    )</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.register_csv_dialect"><code class="name flex">
<span>def <span class="ident">register_csv_dialect</span></span>(<span>)</span>
</code></dt>
<dd>
<div class="desc"><p>Register the KBC CSV dialect</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def register_csv_dialect():
    &#34;&#34;&#34;
    Register the KBC CSV dialect
    &#34;&#34;&#34;
    csv.register_dialect(&#39;kbc&#39;, lineterminator=&#39;\n&#39;, delimiter=&#39;,&#39;,
                         quotechar=&#39;&#34;&#39;)</code></pre>
</details>
</dd>
</dl>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="keboola.component.interface.CommonInterface"><code class="flex name class">
<span>class <span class="ident">CommonInterface</span></span>
<span>(</span><span>data_folder_path:str=None, log_level=20, logging_type=None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class handling standard tasks related to the
<a href="https://developers.keboola.com/extend/common-interface/">Keboola Common Interface</a>
e.g. config load, validation, component state, I/O handling, I/O metadata and manifest files.</p>
<p>It initializes the environment inject into the Docker container the KBC component runs in and abstracts the tasks
related to the Common Interface interaction.</p>
<h2 id="attributes">Attributes</h2>
<p>data_folder_path (str):
Full path to the /data folder
Initializes the CommonInterface environment. If the data_folder_path is not specified the folder
is established in following order:</p>
<ul>
<li>From provided argument if present: <code>-d</code> or <code>--data</code></li>
<li>From environment variable if present (KBC_DATADIR)</li>
<li>Defaults to /data/ if none of the above is specified</li>
</ul>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>data_folder_path</code></strong> :&ensp;<code>str</code></dt>
<dd>path to a data folder.</dd>
<dt><strong><code>log_level</code></strong> :&ensp;<code>int</code></dt>
<dd>logging.INFO or logging.DEBUG</dd>
<dt><strong><code>logging_type</code></strong> :&ensp;<code>str</code></dt>
<dd>optional 'std' or 'gelf', if left empty determined automatically</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class CommonInterface:
    &#34;&#34;&#34;
    A class handling standard tasks related to the
    [Keboola Common Interface](https://developers.keboola.com/extend/common-interface/)
    e.g. config load, validation, component state, I/O handling, I/O metadata and manifest files.

    It initializes the environment inject into the Docker container the KBC component runs in and abstracts the tasks
    related to the Common Interface interaction.

    Attributes:
        data_folder_path (str):
            Full path to the /data folder

    &#34;&#34;&#34;
    LOGGING_TYPE_STD = &#39;std&#39;
    LOGGING_TYPE_GELF = &#39;gelf&#39;

    def __init__(self, data_folder_path: str = None, log_level=logging.INFO, logging_type=None):
        &#34;&#34;&#34;
        Initializes the CommonInterface environment. If the data_folder_path is not specified the folder
        is established in following order:

        - From provided argument if present: `-d` or `--data`
        - From environment variable if present (KBC_DATADIR)
        - Defaults to /data/ if none of the above is specified

        Args:
            data_folder_path (str): path to a data folder.
            log_level (int): logging.INFO or logging.DEBUG
            logging_type (str): optional &#39;std&#39; or &#39;gelf&#39;, if left empty determined automatically
        &#34;&#34;&#34;
        self.environment_variables = init_environment_variables()
        register_csv_dialect()

        # init logging
        logging_type_inf = CommonInterface.LOGGING_TYPE_GELF if os.getenv(&#39;KBC_LOGGER_ADDR&#39;,
                                                                          None) else CommonInterface.LOGGING_TYPE_STD
        if not logging_type:
            logging_type = logging_type_inf

        if logging_type == CommonInterface.LOGGING_TYPE_STD:
            self.set_default_logger(log_level)
        elif logging_type == CommonInterface.LOGGING_TYPE_GELF:
            self.set_gelf_logger(log_level)

        # init data folder
        if not data_folder_path:
            data_folder_path = self._get_data_folder_from_context()

        # validate
        if not os.path.exists(data_folder_path) and not os.path.isdir(data_folder_path):
            raise ValueError(
                f&#34;The data directory does not exist, verify that the data directory is correct. Dir: &#34;
                f&#34;{data_folder_path}&#34;
            )

        self.data_folder_path = data_folder_path

    def _get_data_folder_from_context(self):
        # try to get from argument parameter

        # get from parameters
        argparser = argparse.ArgumentParser()
        argparser.add_argument(
            &#39;-d&#39;,
            &#39;--data&#39;,
            dest=&#39;data_dir&#39;,
            default=&#39;&#39;,
            help=&#39;Data directory&#39;
        )
        # unknown is to ignore extra arguments
        args, unknown = argparser.parse_known_args()
        data_folder_path = args.data_dir

        if data_folder_path == &#39;&#39; and self.environment_variables.data_dir:
            data_folder_path = self.environment_variables.data_dir
        elif data_folder_path == &#39;&#39;:
            data_folder_path = &#39;/data/&#39;

        return data_folder_path

    # ================================= Logging ==============================
    @staticmethod
    def set_default_logger(log_level: int = logging.INFO):  # noqa: E301
        &#34;&#34;&#34;
        Sets default console logger.

        Args:
            log_level: logging level, default: &#39;logging.INFO&#39;

        Returns:
            Logger object

        &#34;&#34;&#34;

        class InfoFilter(logging.Filter):
            def filter(self, rec):
                return rec.levelno in (logging.DEBUG, logging.INFO)

        hd1 = logging.StreamHandler(sys.stdout)
        hd1.addFilter(InfoFilter())
        hd2 = logging.StreamHandler(sys.stderr)
        hd2.setLevel(logging.WARNING)

        logging.getLogger().setLevel(log_level)
        # remove default handler
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        logging.getLogger().addHandler(hd1)
        logging.getLogger().addHandler(hd2)

        logger = logging.getLogger()
        return logger

    @staticmethod
    def set_gelf_logger(log_level: int = logging.INFO, transport_layer=&#39;TCP&#39;,
                        stdout=False, include_extra_fields=True, **gelf_kwargs):  # noqa: E301
        &#34;&#34;&#34;
        Sets gelf console logger. Handler for console output is not included by default,
        for testing in non-gelf environments use stdout=True.

        Args:
            log_level: logging level, default: &#39;logging.INFO&#39;
            transport_layer: &#39;TCP&#39; or &#39;UDP&#39;, default:&#39;UDP
            stdout: if set to True, Stout handler is also included
            include_extra_fields:
                Include extra GELF fields in the log messages.
                e.g. logging.warning(&#39;Some warning&#39;,
                                     extra={&#34;additional_info&#34;: &#34;Extra info to be displayed in the detail&#34;}

        Returns: Logger object
        &#34;&#34;&#34;
        # remove existing handlers
        for h in logging.getLogger().handlers:
            logging.getLogger().removeHandler(h)
        if stdout:
            CommonInterface.set_default_logger(log_level)

        # gelf handler setup
        gelf_kwargs[&#39;include_extra_fields&#39;] = include_extra_fields

        host = os.getenv(&#39;KBC_LOGGER_ADDR&#39;, &#39;localhost&#39;)
        port = os.getenv(&#39;KBC_LOGGER_PORT&#39;, 12201)
        if transport_layer == &#39;TCP&#39;:
            gelf = GelfTcpHandler(host=host, port=port, **gelf_kwargs)
        elif transport_layer == &#39;UDP&#39;:
            gelf = GelfUdpHandler(host=host, port=port, **gelf_kwargs)
        else:
            raise ValueError(F&#39;Unsupported gelf transport layer: {transport_layer}. Choose TCP or UDP&#39;)

        logging.getLogger().setLevel(log_level)
        logging.getLogger().addHandler(gelf)

        logger = logging.getLogger()
        return logger

    def get_state_file(self) -&gt; dict:
        &#34;&#34;&#34;

        Returns dict representation of state file or empty dict if not present

        Returns:
            dict:

        &#34;&#34;&#34;
        logging.info(&#39;Loading state file..&#39;)
        state_file_path = os.path.join(self.data_folder_path, &#39;in&#39;, &#39;state.json&#39;)
        if not os.path.isfile(state_file_path):
            logging.info(&#39;State file not found. First run?&#39;)
            return {}
        try:
            with open(state_file_path, &#39;r&#39;) \
                    as state_file:
                return json.load(state_file)
        except (OSError, IOError):
            raise ValueError(
                &#34;State file state.json unable to read &#34;
            )

    def write_state_file(self, state_dict: dict):
        &#34;&#34;&#34;
        Stores [state file](https://developers.keboola.com/extend/common-interface/config-file/#state-file).
        Args:
            state_dict (dict):
        &#34;&#34;&#34;
        if not isinstance(state_dict, dict):
            raise TypeError(&#39;Dictionary expected as a state file datatype!&#39;)

        with open(os.path.join(self.configuration.data_dir, &#39;out&#39;, &#39;state.json&#39;), &#39;w+&#39;) as state_file:
            json.dump(state_dict, state_file)

    def get_input_table_definition_by_name(self, table_name: str) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
        Return dao.TableDefinition object by table name.

        If nor the table itself or it&#39;s manifest exists, a ValueError is thrown.

        The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
        provides methods for updating the manifest metadata.

        Args:
            table_name: Destination table name (name of .csv file). e.g. input.csv

        Returns:
            dao.TableDefinition
        &#34;&#34;&#34;
        manifest_path = os.path.join(
            self.tables_in_path,
            table_name + &#39;.manifest&#39;
        )

        return dao.TableDefinition.build_from_manifest(manifest_path)

    def get_input_tables_definitions(self, orphaned_manifests=False) -&gt; List[dao.TableDefinition]:
        &#34;&#34;&#34;
        Return dao.TableDefinition objects by scanning the `data/in/tables` folder.

        The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
        provides methods for updating the manifest metadata.

        By default, orphaned manifests are skipped.


        See Also: keboola.component.dao.dao.TableDefinition

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. This is useful in
            in scenarios where [workspaces exchange](
            https://developers.keboola.com/extend/common-interface/folders/#exchanging-data-via-workspace) is used
            e.g. when only manifest files are present in the `data/in/tables` folder.

        Returns: List[dao.TableDefinition]

        &#34;&#34;&#34;

        table_files = [f for f in glob.glob(self.tables_in_path + &#34;/**&#34;, recursive=False) if
                       not f.endswith(&#39;.manifest&#39;)]
        table_defs = list()
        for t in table_files:
            p = Path(t)
            manifest_path = t + &#39;.manifest&#39;

            if p.is_dir() and not Path(manifest_path).exists():
                # skip folders that do not have matching manifest
                logging.warning(f&#39;Folder {t} does not have matching manifest, it will be ignored!&#39;)
                continue

            table_defs.append(dao.TableDefinition.build_from_manifest(manifest_path))

        if orphaned_manifests:
            files_w_manifest = [t.name + &#39;.manifest&#39; for t in table_defs]
            manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                              if Path(f).name not in files_w_manifest]
            for t in manifest_files:
                p = Path(t)

                if p.is_dir():
                    # skip folders that do not have matching manifest
                    logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                    continue

                table_defs.append(dao.TableDefinition.build_from_manifest(t))
        return table_defs

    def _create_table_definition(self, name: str,
                                 storage_stage: str = &#39;out&#39;,
                                 is_sliced: bool = False,
                                 destination: str = &#39;&#39;,
                                 primary_key: List[str] = None,
                                 columns: List[str] = None,
                                 incremental: bool = None,
                                 table_metadata: dao.TableMetadata = None,
                                 enclosure: str = &#39;&#34;&#39;,
                                 delimiter: str = &#39;,&#39;,
                                 delete_where: dict = None,
                                 write_always: bool = False) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                Helper method for dao.TableDefinition creation along with the &#34;manifest&#34;.
                It initializes path according to the storage_stage type.

                Args:
                    name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                    storage_stage:
                        default value: &#39;out&#39;
                        either `&#39;in&#39;` or `&#39;out&#39;`. Determines the path to result file.
                        E.g. `data/tables/in/my_table.csv`
                    is_sliced: True if the full_path points to a folder with sliced tables
                    destination: String name of the table in Storage.
                    primary_key: List with names of columns used for primary key.
                    columns: List of columns for headless CSV files
                    incremental: Set to true to enable incremental loading
                    table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                    enclosure: str: CSV enclosure, by default &#34;
                    delimiter: str: CSV delimiter, by default ,
                    delete_where: Dict with settings for deleting rows
                    write_always: Bool: If true, the table will be saved to Storage even when the job execution
                           fails.
        &#34;&#34;&#34;
        if storage_stage == &#39;in&#39;:
            full_path = os.path.join(self.tables_in_path, name)
        elif storage_stage == &#39;out&#39;:
            full_path = os.path.join(self.tables_out_path, name)
        else:
            raise ValueError(f&#39;Invalid storage_stage value &#34;{storage_stage}&#34;. Supported values are: &#34;in&#34; or &#34;out&#34;!&#39;)

        return dao.TableDefinition(name=name,
                                   full_path=full_path,
                                   is_sliced=is_sliced,
                                   destination=destination,
                                   primary_key=primary_key,
                                   columns=columns,
                                   incremental=incremental,
                                   table_metadata=table_metadata,
                                   enclosure=enclosure,
                                   delimiter=delimiter,
                                   delete_where=delete_where,
                                   stage=storage_stage,
                                   write_always=write_always)

    def create_in_table_definition(self, name: str,
                                   is_sliced: bool = False,
                                   destination: str = &#39;&#39;,
                                   primary_key: List[str] = None,
                                   columns: List[str] = None,
                                   incremental: bool = None,
                                   table_metadata: dao.TableMetadata = None,
                                   delete_where: str = None) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                       Helper method for input dao.TableDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/tables/in/ folder.

                       Args:
                           name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                           is_sliced: True if the full_path points to a folder with sliced tables
                           destination: String name of the table in Storage.
                           primary_key: List with names of columns used for primary key.
                           columns: List of columns for headless CSV files
                           incremental: Set to true to enable incremental loading
                           table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                           delete_where: Dict with settings for deleting rows
        &#34;&#34;&#34;

        return self._create_table_definition(name=name,
                                             storage_stage=&#39;in&#39;,
                                             is_sliced=is_sliced,
                                             destination=destination,
                                             primary_key=primary_key,
                                             columns=columns,
                                             incremental=incremental,
                                             table_metadata=table_metadata,
                                             delete_where=delete_where)

    def create_out_table_definition(self, name: str,
                                    is_sliced: bool = False,
                                    destination: str = &#39;&#39;,
                                    primary_key: List[str] = None,
                                    columns: List[str] = None,
                                    incremental: bool = None,
                                    table_metadata: dao.TableMetadata = None,
                                    enclosure: str = &#39;&#34;&#39;,
                                    delimiter: str = &#39;,&#39;,
                                    delete_where: dict = None,
                                    write_always: bool = False) -&gt; dao.TableDefinition:
        &#34;&#34;&#34;
                       Helper method for output dao.TableDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/tables/out/ folder.

                       Args:
                           name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                           is_sliced: True if the full_path points to a folder with sliced tables
                           destination: String name of the table in Storage.
                           primary_key: List with names of columns used for primary key.
                           columns: List of columns for headless CSV files
                           incremental: Set to true to enable incremental loading
                           table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                           enclosure: str: CSV enclosure, by default &#34;
                           delimiter: str: CSV delimiter, by default ,
                           delete_where: Dict with settings for deleting rows
                           write_always: Bool: If true, the table will be saved to Storage even when the job execution
                           fails.
        &#34;&#34;&#34;

        return self._create_table_definition(name=name,
                                             storage_stage=&#39;out&#39;,
                                             is_sliced=is_sliced,
                                             destination=destination,
                                             primary_key=primary_key,
                                             columns=columns,
                                             incremental=incremental,
                                             table_metadata=table_metadata,
                                             enclosure=enclosure,
                                             delimiter=delimiter,
                                             delete_where=delete_where,
                                             write_always=write_always)

    # # File processing

    def get_input_file_definitions_grouped_by_tag_group(self, orphaned_manifests=False,
                                                        only_latest_files=True,
                                                        tags: List[str] = None,
                                                        include_system_tags=False) \
            -&gt; Dict[str, List[dao.FileDefinition]]:
        &#34;&#34;&#34;
        Convenience method returning lists of files in dictionary grouped by tag group.

        (tag group is string built from alphabetically ordered and concatenated tags, e.g. &#39;tag1;tag2&#39;

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                    a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files containing specified tags will be fetched.
            include_system_tags (bool): optional flag that will use system generated tags in groups as well.
                                   See FileDefinition.SYSTEM_TAG_PREFIXES

        Returns:
            Dict[str,List[dao.FileDefinition]] indexed by tag group =&gt; string built from alphabetically ordered
            and concatenated tags, e.g. `tag1;tag2`

        &#34;&#34;&#34;
        file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
        return self.__group_file_defs_by_tag_group(file_definitions, include_system_tags=include_system_tags)

    def get_input_file_definitions_grouped_by_name(self, orphaned_manifests=False, only_latest_files=True,
                                                   tags: List[str] = None) -&gt; Dict[str, List[dao.FileDefinition]]:
        &#34;&#34;&#34;
        Convenience method returning lists of files in dictionary grouped by file name.

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                    a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

        Returns:

        &#34;&#34;&#34;
        file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
        return self.__group_files_by_name(file_definitions)

    def __group_file_defs_by_tag_group(self, file_definitions: List[dao.FileDefinition], include_system_tags=False) \
            -&gt; Dict[str, List[dao.FileDefinition]]:

        files_per_tag: dict = {}
        for f in file_definitions:

            tag_group_v1 = f.tags if include_system_tags else f.user_tags
            tag_group_v1.sort()
            tag_group_key = &#39;;&#39;.join(tag_group_v1)
            if not files_per_tag.get(tag_group_key):
                files_per_tag[tag_group_key] = []
            files_per_tag[tag_group_key].append(f)
        return files_per_tag

    def _filter_files(self, file_definitions: List[dao.FileDefinition], tags: List[str] = None,
                      only_latest: bool = True) -&gt; List[dao.FileDefinition]:

        filtered_files = file_definitions

        if only_latest:
            filtered_files = self.__filter_filedefs_by_latest(filtered_files)

        # filter by tags
        if tags:
            new_filtered = []
            filter_set = set(tags)
            for fd in filtered_files:
                tags_set = set(fd.tags)
                if filter_set.issubset(tags_set):
                    new_filtered.append(fd)
            filtered_files = new_filtered

        return filtered_files

    def __group_files_by_name(self, file_definitions: List[dao.FileDefinition]) -&gt; Dict[str, List[dao.FileDefinition]]:
        files_per_name: dict = {}
        for f in file_definitions:
            if not files_per_name.get(f.name):
                files_per_name[f.name] = []
            files_per_name[f.name].append(f)
        return files_per_name

    def __filter_filedefs_by_latest(self, file_definitions: List[dao.FileDefinition]) -&gt; List[dao.FileDefinition]:
        &#34;&#34;&#34;
        Get latest file (according to the timestamp) by each filename
        Args:
            file_definitions:

        Returns:

        &#34;&#34;&#34;
        filtered_files = list()
        files_per_name = self.__group_files_by_name(file_definitions)
        for group in files_per_name:
            max_file = None
            max_timestamp = utc.localize(datetime(1900, 5, 17))
            for f in files_per_name[group]:
                creation_date = f.created
                # if date not present ignore and add anyway
                if creation_date is None or creation_date &gt; max_timestamp:
                    max_timestamp = creation_date
                    max_file = f
            filtered_files.append(max_file)
        return filtered_files

    def get_input_files_definitions(self, orphaned_manifests=False,
                                    only_latest_files=True,
                                    tags: Optional[List[str]] = None) -&gt; List[dao.FileDefinition]:
        &#34;&#34;&#34;
        Return dao.FileDefinition objects by scanning the `data/in/files` folder.

        The dao.FileDefinition will contain full path of the source file, it&#39;s name and manifest.

        By default only latest versions of each file are included.

        By default, orphaned manifests are skipped, otherwise fails with ValueError.

        A filter may be specified to match only some tags. All files containing specified tags will be returned.


        See Also: keboola.component.dao.FileDefinition

        Args:
            orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
            a ValueError is raised.
            only_latest_files (bool): If True, only latest versions of each files are included.
            tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

        Returns: List[dao.TableDefinition]

        &#34;&#34;&#34;

        in_files = [f for f in glob.glob(self.files_in_path + &#34;/**&#34;, recursive=False) if
                    not f.endswith(&#39;.manifest&#39;)]
        file_defs = list()
        for t in in_files:
            manifest_path = t + &#39;.manifest&#39;

            file_defs.append(dao.FileDefinition.build_from_manifest(manifest_path))

        if orphaned_manifests:
            files_w_manifest = [t.full_path for t in file_defs]
            manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                              if Path(f).name not in files_w_manifest]
            for t in manifest_files:
                p = Path(t)

                if p.is_dir():
                    # skip folders that do not have matching manifest
                    logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                    continue

                file_defs.append(dao.FileDefinition.build_from_manifest(t))

        return self._filter_files(file_defs, tags, only_latest_files)

    def _create_file_definition(self,
                                name: str,
                                storage_stage: str = &#39;out&#39;,
                                tags: List[str] = None,
                                is_public: bool = False,
                                is_permanent: bool = False,
                                is_encrypted: bool = False,
                                notify: bool = False) -&gt; dao.FileDefinition:
        &#34;&#34;&#34;
                Helper method for dao.FileDefinition creation along with the &#34;manifest&#34;.
                It initializes path according to the storage_stage type.

                Args:
                                name (str): Name of the file, e.g. file.jpg.
                                tags (list):
                                    List of tags that are assigned to this file
                                is_public: When true, the file URL will be permanent and publicly accessible.
                                is_permanent: Keeps a file forever. If false, the file will be deleted after default
                                period of time (e.g.
                                15 days)
                                is_encrypted: If true, the file content will be encrypted in the storage.
                                notify: Notifies project administrators that a file was uploaded.
        &#34;&#34;&#34;
        if storage_stage == &#39;in&#39;:
            full_path = os.path.join(self.files_in_path, name)
        elif storage_stage == &#39;out&#39;:
            full_path = os.path.join(self.files_out_path, name)
        else:
            raise ValueError(f&#39;Invalid storage_stage value &#34;{storage_stage}&#34;. Supported values are: &#34;in&#34; or &#34;out&#34;!&#39;)

        return dao.FileDefinition(
            full_path=full_path,
            tags=tags,
            is_public=is_public,
            is_permanent=is_permanent,
            is_encrypted=is_encrypted,
            notify=notify)

    def create_out_file_definition(self, name: str,
                                   tags: List[str] = None,
                                   is_public: bool = False,
                                   is_permanent: bool = False,
                                   is_encrypted: bool = False,
                                   notify: bool = False) -&gt; dao.FileDefinition:
        &#34;&#34;&#34;
                       Helper method for input dao.FileDefinition creation along with the &#34;manifest&#34;.
                       It initializes path in data/files/out/ folder.

                       Args:
                                name (str): Name of the file, e.g. file.jpg.
                                tags (list):
                                    List of tags that are assigned to this file
                                is_public: When true, the file URL will be permanent and publicly accessible.
                                is_permanent: Keeps a file forever. If false, the file will be deleted after default
                                period of time (e.g.
                                15 days)
                                is_encrypted: If true, the file content will be encrypted in the storage.
                                notify: Notifies project administrators that a file was uploaded.
        &#34;&#34;&#34;

        return self._create_file_definition(name=name,
                                            storage_stage=&#39;out&#39;,
                                            tags=tags,
                                            is_public=is_public,
                                            is_permanent=is_permanent,
                                            is_encrypted=is_encrypted,
                                            notify=notify)

    # TODO: refactor the validate config so it&#39;s more userfriendly
    &#34;&#34;&#34;
        - Support for nested params?
    &#34;&#34;&#34;

    def validate_configuration_parameters(self, mandatory_params=None):
        &#34;&#34;&#34;
                Validates config parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                =&gt; at least one of the pars has to be present
                ex2.
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                &#34;&#34;&#34;
        if not mandatory_params:
            mandatory_params = []
        return self._validate_parameters(self.configuration.parameters, mandatory_params, &#39;config parameters&#39;)

    def validate_image_parameters(self, mandatory_params):
        &#34;&#34;&#34;
                Validates image parameters based on provided mandatory parameters.
                All provided parameters must be present in config to pass.
                ex1.:
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                mandatory_params = [par1, par2]
                Validation will fail when one of the above parameters is not found

                Two levels of nesting:
                Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
                =&gt; at least one of the pars has to be present
                ex2.
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group1]

                Folowing logical expression is evaluated:
                Par1 AND Par2 AND (groupPar1 OR groupPar2)

                ex3
                par1 = &#39;par1&#39;
                par2 = &#39;par2&#39;
                par3 = &#39;par3&#39;
                groupPar1 = &#39;groupPar1&#39;
                groupPar2 = &#39;groupPar2&#39;
                group1 = [groupPar1, groupPar2]
                group3 = [par3, group1]
                mandatory_params = [par1, par2, group3]

                Following logical expression is evaluated:
                par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
                &#34;&#34;&#34;
        return self._validate_parameters(self.configuration.image_parameters,
                                         mandatory_params, &#39;image/stack parameters&#39;)

    def _validate_parameters(self, parameters, mandatory_params, _type):
        &#34;&#34;&#34;
        Validates provided parameters based on provided mandatory parameters.
        All provided parameters must be present in config to pass.
        ex1.:
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        mandatory_params = [par1, par2]
        Validation will fail when one of the above parameters is not found

        Two levels of nesting:
        Parameters can be grouped as arrays par3 = [groupPar1, groupPar2] =&gt; at least one of the pars has to be
        present
        ex2.
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        par3 = &#39;par3&#39;
        groupPar1 = &#39;groupPar1&#39;
        groupPar2 = &#39;groupPar2&#39;
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group1]

        Folowing logical expression is evaluated:
        Par1 AND Par2 AND (groupPar1 OR groupPar2)

        ex3
        par1 = &#39;par1&#39;
        par2 = &#39;par2&#39;
        par3 = &#39;par3&#39;
        groupPar1 = &#39;groupPar1&#39;
        groupPar2 = &#39;groupPar2&#39;
        group1 = [groupPar1, groupPar2]
        group3 = [par3, group1]
        mandatory_params = [par1, par2, group3]

        Following logical expression is evaluated:
        par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))

        Raises:
            UserException: on missing parameters
        &#34;&#34;&#34;
        missing_fields = []
        for par in mandatory_params:
            if isinstance(par, list):
                missing_fields.extend(self._validate_par_group(par, parameters))
            elif parameters.get(par) is None:
                missing_fields.append(par)

        if missing_fields:
            raise UserException(
                &#39;Missing mandatory {} fields: [{}] &#39;.format(_type, &#39;, &#39;.join(missing_fields)))

    def _validate_par_group(self, par_group, parameters):
        missing_fields = []
        is_present = False
        for par in par_group:
            if isinstance(par, list):
                missing_subset = self._get_par_missing_fields(par, parameters)
                missing_fields.extend(missing_subset)
                if not missing_subset:
                    is_present = True

            elif parameters.get(par):
                is_present = True
            else:
                missing_fields.append(par)
        if not is_present:
            return missing_fields
        else:
            return []

    def _get_par_missing_fields(self, mand_params, parameters):
        missing_fields = []
        for par in mand_params:
            if not parameters.get(par):
                missing_fields.append(par)
        return missing_fields

    # ### PROPERTIES
    @property
    def configuration(self):
        # try to load the configuration
        # raises ValueError

        return Configuration(self.data_folder_path)

    @property
    def tables_out_path(self):
        return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;tables&#39;)

    @property
    def tables_in_path(self):
        return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;tables&#39;)

    @property
    def files_out_path(self):
        return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;files&#39;)

    @property
    def files_in_path(self):
        return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;files&#39;)

    @property
    def is_legacy_queue(self) -&gt; bool:
        &#34;&#34;&#34;
        Check if the project is running on legacy queue (v1)
        Returns:

        &#34;&#34;&#34;
        features = os.environ.get(&#39;KBC_PROJECT_FEATURE_GATES&#39;)
        is_legacy_queue = True
        if not features or &#39;queuev2&#39; in features:
            is_legacy_queue = False
        return is_legacy_queue

    def write_manifest(self, io_definition: Union[dao.FileDefinition, dao.TableDefinition]):
        &#34;&#34;&#34;
        Write a table manifest from dao.IODefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()

        # build table definition
        table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                                , incremental = True
                                , table_metadata = tm
                                ))
        ci.write_manifest(table_def)

        # build file definition
        file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
        ci.write_manifest(file_def)
        ```

        Args:
            io_definition Union[dao.FileDefinition, dao.TableDefinition]: Initialized dao.IODefinition
             object containing manifest.

        Returns:

        &#34;&#34;&#34;

        manifest = io_definition.get_manifest_dictionary(legacy_queue=self.is_legacy_queue)
        # make dirs if not exist
        os.makedirs(os.path.dirname(io_definition.full_path), exist_ok=True)
        with open(io_definition.full_path + &#39;.manifest&#39;, &#39;w&#39;) as manifest_file:
            json.dump(manifest, manifest_file)

    def write_manifests(self, io_definitions: List[Union[dao.FileDefinition, dao.TableDefinition]]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            io_definitions:

        Returns:

        &#34;&#34;&#34;
        for io_def in io_definitions:
            self.write_manifest(io_def)

    # ############# DEPRECATED METHODS, TODO: remove

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
    def write_filedef_manifest(self, file_definition: dao.FileDefinition):
        &#34;&#34;&#34;
        Write a table manifest from dao.FileDefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()

        # build table definition
        file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
        ci.write_filedef_manifest(file_def)
        ```

        Args:
            file_definition (dao.FileDefinition): Initialized dao.FileDefinition object containing manifest.

        Returns:

        &#34;&#34;&#34;
        self.write_manifest(file_definition)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
    def write_filedef_manifests(self, file_definitions: List[dao.FileDefinition]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            file_definitions:

        Returns:

        &#34;&#34;&#34;
        self.write_manifests(file_definitions)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
    def write_tabledef_manifest(self, table_definition: dao.TableDefinition):
        &#34;&#34;&#34;
        Write a table manifest from dao.TableDefinition. Creates the appropriate manifest file in the proper location.


        ** Usage:**

        ```python
        from keboola.component import CommonInterface
        from keboola.component import dao

        ci = CommonInterface()
        tm = dao.TableMetadata()
        tm.add_table_description(&#34;My new table&#34;)

        # build table definition
        table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                                , incremental = True
                                , table_metadata = tm
                                ))
        ci.write_tabledef_manifest(table_def)
        ```

        Args:
            table_definition (dao.TableDefinition): Initialized dao.TableDefinition object containing manifest.

        Returns:

        &#34;&#34;&#34;
        self.write_manifest(table_definition)

    @deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
    def write_tabledef_manifests(self, table_definitions: List[dao.TableDefinition]):
        &#34;&#34;&#34;
        Process all table definition objects and create appropriate manifest files.
        Args:
            table_definitions:

        Returns:

        &#34;&#34;&#34;
        self.write_manifests(table_definitions)</code></pre>
</details>
<h3>Subclasses</h3>
<ul class="hlist">
<li><a title="keboola.component.base.ComponentBase" href="base.html#keboola.component.base.ComponentBase">ComponentBase</a></li>
</ul>
<h3>Class variables</h3>
<dl>
<dt id="keboola.component.interface.CommonInterface.LOGGING_TYPE_GELF"><code class="name">var <span class="ident">LOGGING_TYPE_GELF</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
<dt id="keboola.component.interface.CommonInterface.LOGGING_TYPE_STD"><code class="name">var <span class="ident">LOGGING_TYPE_STD</span></code></dt>
<dd>
<div class="desc"></div>
</dd>
</dl>
<h3>Static methods</h3>
<dl>
<dt id="keboola.component.interface.CommonInterface.set_default_logger"><code class="name flex">
<span>def <span class="ident">set_default_logger</span></span>(<span>log_level:int=20)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets default console logger.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>log_level</code></strong></dt>
<dd>logging level, default: 'logging.INFO'</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Logger object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def set_default_logger(log_level: int = logging.INFO):  # noqa: E301
    &#34;&#34;&#34;
    Sets default console logger.

    Args:
        log_level: logging level, default: &#39;logging.INFO&#39;

    Returns:
        Logger object

    &#34;&#34;&#34;

    class InfoFilter(logging.Filter):
        def filter(self, rec):
            return rec.levelno in (logging.DEBUG, logging.INFO)

    hd1 = logging.StreamHandler(sys.stdout)
    hd1.addFilter(InfoFilter())
    hd2 = logging.StreamHandler(sys.stderr)
    hd2.setLevel(logging.WARNING)

    logging.getLogger().setLevel(log_level)
    # remove default handler
    for h in logging.getLogger().handlers:
        logging.getLogger().removeHandler(h)
    logging.getLogger().addHandler(hd1)
    logging.getLogger().addHandler(hd2)

    logger = logging.getLogger()
    return logger</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.set_gelf_logger"><code class="name flex">
<span>def <span class="ident">set_gelf_logger</span></span>(<span>log_level:int=20, transport_layer='TCP', stdout=False, include_extra_fields=True, **gelf_kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Sets gelf console logger. Handler for console output is not included by default,
for testing in non-gelf environments use stdout=True.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>log_level</code></strong></dt>
<dd>logging level, default: 'logging.INFO'</dd>
<dt><strong><code>transport_layer</code></strong></dt>
<dd>'TCP' or 'UDP', default:'UDP</dd>
<dt><strong><code>stdout</code></strong></dt>
<dd>if set to True, Stout handler is also included</dd>
</dl>
<p>include_extra_fields:
Include extra GELF fields in the log messages.
e.g. logging.warning('Some warning',
extra={"additional_info": "Extra info to be displayed in the detail"}
Returns: Logger object</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@staticmethod
def set_gelf_logger(log_level: int = logging.INFO, transport_layer=&#39;TCP&#39;,
                    stdout=False, include_extra_fields=True, **gelf_kwargs):  # noqa: E301
    &#34;&#34;&#34;
    Sets gelf console logger. Handler for console output is not included by default,
    for testing in non-gelf environments use stdout=True.

    Args:
        log_level: logging level, default: &#39;logging.INFO&#39;
        transport_layer: &#39;TCP&#39; or &#39;UDP&#39;, default:&#39;UDP
        stdout: if set to True, Stout handler is also included
        include_extra_fields:
            Include extra GELF fields in the log messages.
            e.g. logging.warning(&#39;Some warning&#39;,
                                 extra={&#34;additional_info&#34;: &#34;Extra info to be displayed in the detail&#34;}

    Returns: Logger object
    &#34;&#34;&#34;
    # remove existing handlers
    for h in logging.getLogger().handlers:
        logging.getLogger().removeHandler(h)
    if stdout:
        CommonInterface.set_default_logger(log_level)

    # gelf handler setup
    gelf_kwargs[&#39;include_extra_fields&#39;] = include_extra_fields

    host = os.getenv(&#39;KBC_LOGGER_ADDR&#39;, &#39;localhost&#39;)
    port = os.getenv(&#39;KBC_LOGGER_PORT&#39;, 12201)
    if transport_layer == &#39;TCP&#39;:
        gelf = GelfTcpHandler(host=host, port=port, **gelf_kwargs)
    elif transport_layer == &#39;UDP&#39;:
        gelf = GelfUdpHandler(host=host, port=port, **gelf_kwargs)
    else:
        raise ValueError(F&#39;Unsupported gelf transport layer: {transport_layer}. Choose TCP or UDP&#39;)

    logging.getLogger().setLevel(log_level)
    logging.getLogger().addHandler(gelf)

    logger = logging.getLogger()
    return logger</code></pre>
</details>
</dd>
</dl>
<h3>Instance variables</h3>
<dl>
<dt id="keboola.component.interface.CommonInterface.configuration"><code class="name">var <span class="ident">configuration</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def configuration(self):
    # try to load the configuration
    # raises ValueError

    return Configuration(self.data_folder_path)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.files_in_path"><code class="name">var <span class="ident">files_in_path</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def files_in_path(self):
    return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;files&#39;)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.files_out_path"><code class="name">var <span class="ident">files_out_path</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def files_out_path(self):
    return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;files&#39;)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.is_legacy_queue"><code class="name">var <span class="ident">is_legacy_queue</span> :bool</code></dt>
<dd>
<div class="desc"><p>Check if the project is running on legacy queue (v1)
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def is_legacy_queue(self) -&gt; bool:
    &#34;&#34;&#34;
    Check if the project is running on legacy queue (v1)
    Returns:

    &#34;&#34;&#34;
    features = os.environ.get(&#39;KBC_PROJECT_FEATURE_GATES&#39;)
    is_legacy_queue = True
    if not features or &#39;queuev2&#39; in features:
        is_legacy_queue = False
    return is_legacy_queue</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.tables_in_path"><code class="name">var <span class="ident">tables_in_path</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tables_in_path(self):
    return os.path.join(self.data_folder_path, &#39;in&#39;, &#39;tables&#39;)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.tables_out_path"><code class="name">var <span class="ident">tables_out_path</span></code></dt>
<dd>
<div class="desc"></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tables_out_path(self):
    return os.path.join(self.data_folder_path, &#39;out&#39;, &#39;tables&#39;)</code></pre>
</details>
</dd>
</dl>
<h3>Methods</h3>
<dl>
<dt id="keboola.component.interface.CommonInterface.create_in_table_definition"><code class="name flex">
<span>def <span class="ident">create_in_table_definition</span></span>(<span>self, name:str, is_sliced:bool=False, destination:str='', primary_key:List[str]=None, columns:List[str]=None, incremental:bool=None, table_metadata:<a title="keboola.component.dao.TableMetadata" href="dao.html#keboola.component.dao.TableMetadata">TableMetadata</a>=None, delete_where:str=None) ><a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a></span>
</code></dt>
<dd>
<div class="desc"><p>Helper method for input dao.TableDefinition creation along with the "manifest".
It initializes path in data/tables/in/ folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Table / file name. e.g. <code>'my_table.csv'</code>.</dd>
<dt><strong><code>is_sliced</code></strong></dt>
<dd>True if the full_path points to a folder with sliced tables</dd>
<dt><strong><code>destination</code></strong></dt>
<dd>String name of the table in Storage.</dd>
<dt><strong><code>primary_key</code></strong></dt>
<dd>List with names of columns used for primary key.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>List of columns for headless CSV files</dd>
<dt><strong><code>incremental</code></strong></dt>
<dd>Set to true to enable incremental loading</dd>
<dt><strong><code>table_metadata</code></strong></dt>
<dd>&lt;.dao.TableMetadata&gt; object containing column and table metadata</dd>
<dt><strong><code>delete_where</code></strong></dt>
<dd>Dict with settings for deleting rows</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_in_table_definition(self, name: str,
                               is_sliced: bool = False,
                               destination: str = &#39;&#39;,
                               primary_key: List[str] = None,
                               columns: List[str] = None,
                               incremental: bool = None,
                               table_metadata: dao.TableMetadata = None,
                               delete_where: str = None) -&gt; dao.TableDefinition:
    &#34;&#34;&#34;
                   Helper method for input dao.TableDefinition creation along with the &#34;manifest&#34;.
                   It initializes path in data/tables/in/ folder.

                   Args:
                       name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                       is_sliced: True if the full_path points to a folder with sliced tables
                       destination: String name of the table in Storage.
                       primary_key: List with names of columns used for primary key.
                       columns: List of columns for headless CSV files
                       incremental: Set to true to enable incremental loading
                       table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                       delete_where: Dict with settings for deleting rows
    &#34;&#34;&#34;

    return self._create_table_definition(name=name,
                                         storage_stage=&#39;in&#39;,
                                         is_sliced=is_sliced,
                                         destination=destination,
                                         primary_key=primary_key,
                                         columns=columns,
                                         incremental=incremental,
                                         table_metadata=table_metadata,
                                         delete_where=delete_where)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.create_out_file_definition"><code class="name flex">
<span>def <span class="ident">create_out_file_definition</span></span>(<span>self, name:str, tags:List[str]=None, is_public:bool=False, is_permanent:bool=False, is_encrypted:bool=False, notify:bool=False) ><a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a></span>
</code></dt>
<dd>
<div class="desc"><p>Helper method for input dao.FileDefinition creation along with the "manifest".
It initializes path in data/files/out/ folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong> :&ensp;<code>str</code></dt>
<dd>Name of the file, e.g. file.jpg.</dd>
<dt>tags (list):</dt>
<dt>List of tags that are assigned to this file</dt>
<dt><strong><code>is_public</code></strong></dt>
<dd>When true, the file URL will be permanent and publicly accessible.</dd>
<dt><strong><code>is_permanent</code></strong></dt>
<dd>Keeps a file forever. If false, the file will be deleted after default</dd>
<dt>period of time (e.g.</dt>
<dt>15 days)</dt>
<dt><strong><code>is_encrypted</code></strong></dt>
<dd>If true, the file content will be encrypted in the storage.</dd>
<dt><strong><code>notify</code></strong></dt>
<dd>Notifies project administrators that a file was uploaded.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_out_file_definition(self, name: str,
                               tags: List[str] = None,
                               is_public: bool = False,
                               is_permanent: bool = False,
                               is_encrypted: bool = False,
                               notify: bool = False) -&gt; dao.FileDefinition:
    &#34;&#34;&#34;
                   Helper method for input dao.FileDefinition creation along with the &#34;manifest&#34;.
                   It initializes path in data/files/out/ folder.

                   Args:
                            name (str): Name of the file, e.g. file.jpg.
                            tags (list):
                                List of tags that are assigned to this file
                            is_public: When true, the file URL will be permanent and publicly accessible.
                            is_permanent: Keeps a file forever. If false, the file will be deleted after default
                            period of time (e.g.
                            15 days)
                            is_encrypted: If true, the file content will be encrypted in the storage.
                            notify: Notifies project administrators that a file was uploaded.
    &#34;&#34;&#34;

    return self._create_file_definition(name=name,
                                        storage_stage=&#39;out&#39;,
                                        tags=tags,
                                        is_public=is_public,
                                        is_permanent=is_permanent,
                                        is_encrypted=is_encrypted,
                                        notify=notify)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.create_out_table_definition"><code class="name flex">
<span>def <span class="ident">create_out_table_definition</span></span>(<span>self, name:str, is_sliced:bool=False, destination:str='', primary_key:List[str]=None, columns:List[str]=None, incremental:bool=None, table_metadata:<a title="keboola.component.dao.TableMetadata" href="dao.html#keboola.component.dao.TableMetadata">TableMetadata</a>=None, enclosure:str='"', delimiter:str=',', delete_where:dict=None, write_always:bool=False) ><a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a></span>
</code></dt>
<dd>
<div class="desc"><p>Helper method for output dao.TableDefinition creation along with the "manifest".
It initializes path in data/tables/out/ folder.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>name</code></strong></dt>
<dd>Table / file name. e.g. <code>'my_table.csv'</code>.</dd>
<dt><strong><code>is_sliced</code></strong></dt>
<dd>True if the full_path points to a folder with sliced tables</dd>
<dt><strong><code>destination</code></strong></dt>
<dd>String name of the table in Storage.</dd>
<dt><strong><code>primary_key</code></strong></dt>
<dd>List with names of columns used for primary key.</dd>
<dt><strong><code>columns</code></strong></dt>
<dd>List of columns for headless CSV files</dd>
<dt><strong><code>incremental</code></strong></dt>
<dd>Set to true to enable incremental loading</dd>
<dt><strong><code>table_metadata</code></strong></dt>
<dd>&lt;.dao.TableMetadata&gt; object containing column and table metadata</dd>
<dt><strong><code>enclosure</code></strong></dt>
<dd>str: CSV enclosure, by default "</dd>
<dt><strong><code>delimiter</code></strong></dt>
<dd>str: CSV delimiter, by default ,</dd>
<dt><strong><code>delete_where</code></strong></dt>
<dd>Dict with settings for deleting rows</dd>
<dt><strong><code>write_always</code></strong></dt>
<dd>Bool: If true, the table will be saved to Storage even when the job execution</dd>
</dl>
<p>fails.</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def create_out_table_definition(self, name: str,
                                is_sliced: bool = False,
                                destination: str = &#39;&#39;,
                                primary_key: List[str] = None,
                                columns: List[str] = None,
                                incremental: bool = None,
                                table_metadata: dao.TableMetadata = None,
                                enclosure: str = &#39;&#34;&#39;,
                                delimiter: str = &#39;,&#39;,
                                delete_where: dict = None,
                                write_always: bool = False) -&gt; dao.TableDefinition:
    &#34;&#34;&#34;
                   Helper method for output dao.TableDefinition creation along with the &#34;manifest&#34;.
                   It initializes path in data/tables/out/ folder.

                   Args:
                       name: Table / file name. e.g. `&#39;my_table.csv&#39;`.
                       is_sliced: True if the full_path points to a folder with sliced tables
                       destination: String name of the table in Storage.
                       primary_key: List with names of columns used for primary key.
                       columns: List of columns for headless CSV files
                       incremental: Set to true to enable incremental loading
                       table_metadata: &lt;.dao.TableMetadata&gt; object containing column and table metadata
                       enclosure: str: CSV enclosure, by default &#34;
                       delimiter: str: CSV delimiter, by default ,
                       delete_where: Dict with settings for deleting rows
                       write_always: Bool: If true, the table will be saved to Storage even when the job execution
                       fails.
    &#34;&#34;&#34;

    return self._create_table_definition(name=name,
                                         storage_stage=&#39;out&#39;,
                                         is_sliced=is_sliced,
                                         destination=destination,
                                         primary_key=primary_key,
                                         columns=columns,
                                         incremental=incremental,
                                         table_metadata=table_metadata,
                                         enclosure=enclosure,
                                         delimiter=delimiter,
                                         delete_where=delete_where,
                                         write_always=write_always)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_name"><code class="name flex">
<span>def <span class="ident">get_input_file_definitions_grouped_by_name</span></span>(<span>self, orphaned_manifests=False, only_latest_files=True, tags:List[str]=None) >Dict[str,List[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>Convenience method returning lists of files in dictionary grouped by file name.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>orphaned_manifests</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, manifests without corresponding files are fetched. Otherwise
a ValueError is raised.</dd>
<dt><strong><code>only_latest_files</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, only latest versions of each files are included.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>optional list of tags. If specified only files with matching tag group will be fetched.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_file_definitions_grouped_by_name(self, orphaned_manifests=False, only_latest_files=True,
                                               tags: List[str] = None) -&gt; Dict[str, List[dao.FileDefinition]]:
    &#34;&#34;&#34;
    Convenience method returning lists of files in dictionary grouped by file name.

    Args:
        orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                a ValueError is raised.
        only_latest_files (bool): If True, only latest versions of each files are included.
        tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

    Returns:

    &#34;&#34;&#34;
    file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
    return self.__group_files_by_name(file_definitions)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_tag_group"><code class="name flex">
<span>def <span class="ident">get_input_file_definitions_grouped_by_tag_group</span></span>(<span>self, orphaned_manifests=False, only_latest_files=True, tags:List[str]=None, include_system_tags=False) >Dict[str,List[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>]]</span>
</code></dt>
<dd>
<div class="desc"><p>Convenience method returning lists of files in dictionary grouped by tag group.</p>
<p>(tag group is string built from alphabetically ordered and concatenated tags, e.g. 'tag1;tag2'</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>orphaned_manifests</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, manifests without corresponding files are fetched. Otherwise
a ValueError is raised.</dd>
<dt><strong><code>only_latest_files</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, only latest versions of each files are included.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>optional list of tags. If specified only files containing specified tags will be fetched.</dd>
<dt><strong><code>include_system_tags</code></strong> :&ensp;<code>bool</code></dt>
<dd>optional flag that will use system generated tags in groups as well.
See FileDefinition.SYSTEM_TAG_PREFIXES</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>Dict[str,List[dao.FileDefinition]] indexed by tag group =&gt; string built from alphabetically ordered
and concatenated tags, e.g. <code>tag1;tag2</code></p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_file_definitions_grouped_by_tag_group(self, orphaned_manifests=False,
                                                    only_latest_files=True,
                                                    tags: List[str] = None,
                                                    include_system_tags=False) \
        -&gt; Dict[str, List[dao.FileDefinition]]:
    &#34;&#34;&#34;
    Convenience method returning lists of files in dictionary grouped by tag group.

    (tag group is string built from alphabetically ordered and concatenated tags, e.g. &#39;tag1;tag2&#39;

    Args:
        orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
                a ValueError is raised.
        only_latest_files (bool): If True, only latest versions of each files are included.
        tags (List[str]): optional list of tags. If specified only files containing specified tags will be fetched.
        include_system_tags (bool): optional flag that will use system generated tags in groups as well.
                               See FileDefinition.SYSTEM_TAG_PREFIXES

    Returns:
        Dict[str,List[dao.FileDefinition]] indexed by tag group =&gt; string built from alphabetically ordered
        and concatenated tags, e.g. `tag1;tag2`

    &#34;&#34;&#34;
    file_definitions = self.get_input_files_definitions(orphaned_manifests, only_latest_files, tags)
    return self.__group_file_defs_by_tag_group(file_definitions, include_system_tags=include_system_tags)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_input_files_definitions"><code class="name flex">
<span>def <span class="ident">get_input_files_definitions</span></span>(<span>self, orphaned_manifests=False, only_latest_files=True, tags:Optional[List[str]]=None) >List[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Return dao.FileDefinition objects by scanning the <code>data/in/files</code> folder.</p>
<p>The dao.FileDefinition will contain full path of the source file, it's name and manifest.</p>
<p>By default only latest versions of each file are included.</p>
<p>By default, orphaned manifests are skipped, otherwise fails with ValueError.</p>
<p>A filter may be specified to match only some tags. All files containing specified tags will be returned.</p>
<p>See Also: keboola.component.dao.FileDefinition</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>orphaned_manifests</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, manifests without corresponding files are fetched. Otherwise</dd>
<dt>a ValueError is raised.</dt>
<dt><strong><code>only_latest_files</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, only latest versions of each files are included.</dd>
<dt><strong><code>tags</code></strong> :&ensp;<code>List[str]</code></dt>
<dd>optional list of tags. If specified only files with matching tag group will be fetched.</dd>
</dl>
<p>Returns: List[dao.TableDefinition]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_files_definitions(self, orphaned_manifests=False,
                                only_latest_files=True,
                                tags: Optional[List[str]] = None) -&gt; List[dao.FileDefinition]:
    &#34;&#34;&#34;
    Return dao.FileDefinition objects by scanning the `data/in/files` folder.

    The dao.FileDefinition will contain full path of the source file, it&#39;s name and manifest.

    By default only latest versions of each file are included.

    By default, orphaned manifests are skipped, otherwise fails with ValueError.

    A filter may be specified to match only some tags. All files containing specified tags will be returned.


    See Also: keboola.component.dao.FileDefinition

    Args:
        orphaned_manifests (bool): If True, manifests without corresponding files are fetched. Otherwise
        a ValueError is raised.
        only_latest_files (bool): If True, only latest versions of each files are included.
        tags (List[str]): optional list of tags. If specified only files with matching tag group will be fetched.

    Returns: List[dao.TableDefinition]

    &#34;&#34;&#34;

    in_files = [f for f in glob.glob(self.files_in_path + &#34;/**&#34;, recursive=False) if
                not f.endswith(&#39;.manifest&#39;)]
    file_defs = list()
    for t in in_files:
        manifest_path = t + &#39;.manifest&#39;

        file_defs.append(dao.FileDefinition.build_from_manifest(manifest_path))

    if orphaned_manifests:
        files_w_manifest = [t.full_path for t in file_defs]
        manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                          if Path(f).name not in files_w_manifest]
        for t in manifest_files:
            p = Path(t)

            if p.is_dir():
                # skip folders that do not have matching manifest
                logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                continue

            file_defs.append(dao.FileDefinition.build_from_manifest(t))

    return self._filter_files(file_defs, tags, only_latest_files)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_input_table_definition_by_name"><code class="name flex">
<span>def <span class="ident">get_input_table_definition_by_name</span></span>(<span>self, table_name:str) ><a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a></span>
</code></dt>
<dd>
<div class="desc"><p>Return dao.TableDefinition object by table name.</p>
<p>If nor the table itself or it's manifest exists, a ValueError is thrown.</p>
<p>The dao.TableDefinition will contain full path of the source file, it's name and manifest (if present). It also
provides methods for updating the manifest metadata.</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>table_name</code></strong></dt>
<dd>Destination table name (name of .csv file). e.g. input.csv</dd>
</dl>
<h2 id="returns">Returns</h2>
<p>dao.TableDefinition</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_table_definition_by_name(self, table_name: str) -&gt; dao.TableDefinition:
    &#34;&#34;&#34;
    Return dao.TableDefinition object by table name.

    If nor the table itself or it&#39;s manifest exists, a ValueError is thrown.

    The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
    provides methods for updating the manifest metadata.

    Args:
        table_name: Destination table name (name of .csv file). e.g. input.csv

    Returns:
        dao.TableDefinition
    &#34;&#34;&#34;
    manifest_path = os.path.join(
        self.tables_in_path,
        table_name + &#39;.manifest&#39;
    )

    return dao.TableDefinition.build_from_manifest(manifest_path)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_input_tables_definitions"><code class="name flex">
<span>def <span class="ident">get_input_tables_definitions</span></span>(<span>self, orphaned_manifests=False) >List[<a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a>]</span>
</code></dt>
<dd>
<div class="desc"><p>Return dao.TableDefinition objects by scanning the <code>data/in/tables</code> folder.</p>
<p>The dao.TableDefinition will contain full path of the source file, it's name and manifest (if present). It also
provides methods for updating the manifest metadata.</p>
<p>By default, orphaned manifests are skipped.</p>
<p>See Also: keboola.component.dao.dao.TableDefinition</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>orphaned_manifests</code></strong> :&ensp;<code>bool</code></dt>
<dd>If True, manifests without corresponding files are fetched. This is useful in</dd>
</dl>
<p>in scenarios where <a href="https://developers.keboola.com/extend/common-interface/folders/#exchanging-data-via-workspace">workspaces exchange</a> is used
e.g. when only manifest files are present in the <code>data/in/tables</code> folder.
Returns: List[dao.TableDefinition]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_input_tables_definitions(self, orphaned_manifests=False) -&gt; List[dao.TableDefinition]:
    &#34;&#34;&#34;
    Return dao.TableDefinition objects by scanning the `data/in/tables` folder.

    The dao.TableDefinition will contain full path of the source file, it&#39;s name and manifest (if present). It also
    provides methods for updating the manifest metadata.

    By default, orphaned manifests are skipped.


    See Also: keboola.component.dao.dao.TableDefinition

    Args:
        orphaned_manifests (bool): If True, manifests without corresponding files are fetched. This is useful in
        in scenarios where [workspaces exchange](
        https://developers.keboola.com/extend/common-interface/folders/#exchanging-data-via-workspace) is used
        e.g. when only manifest files are present in the `data/in/tables` folder.

    Returns: List[dao.TableDefinition]

    &#34;&#34;&#34;

    table_files = [f for f in glob.glob(self.tables_in_path + &#34;/**&#34;, recursive=False) if
                   not f.endswith(&#39;.manifest&#39;)]
    table_defs = list()
    for t in table_files:
        p = Path(t)
        manifest_path = t + &#39;.manifest&#39;

        if p.is_dir() and not Path(manifest_path).exists():
            # skip folders that do not have matching manifest
            logging.warning(f&#39;Folder {t} does not have matching manifest, it will be ignored!&#39;)
            continue

        table_defs.append(dao.TableDefinition.build_from_manifest(manifest_path))

    if orphaned_manifests:
        files_w_manifest = [t.name + &#39;.manifest&#39; for t in table_defs]
        manifest_files = [f for f in glob.glob(self.tables_in_path + &#34;/**.manifest&#34;, recursive=False)
                          if Path(f).name not in files_w_manifest]
        for t in manifest_files:
            p = Path(t)

            if p.is_dir():
                # skip folders that do not have matching manifest
                logging.warning(f&#39;Manifest {t} is folder,s skipping!&#39;)
                continue

            table_defs.append(dao.TableDefinition.build_from_manifest(t))
    return table_defs</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.get_state_file"><code class="name flex">
<span>def <span class="ident">get_state_file</span></span>(<span>self) >dict</span>
</code></dt>
<dd>
<div class="desc"><p>Returns dict representation of state file or empty dict if not present</p>
<h2 id="returns">Returns</h2>
<p>dict:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_state_file(self) -&gt; dict:
    &#34;&#34;&#34;

    Returns dict representation of state file or empty dict if not present

    Returns:
        dict:

    &#34;&#34;&#34;
    logging.info(&#39;Loading state file..&#39;)
    state_file_path = os.path.join(self.data_folder_path, &#39;in&#39;, &#39;state.json&#39;)
    if not os.path.isfile(state_file_path):
        logging.info(&#39;State file not found. First run?&#39;)
        return {}
    try:
        with open(state_file_path, &#39;r&#39;) \
                as state_file:
            return json.load(state_file)
    except (OSError, IOError):
        raise ValueError(
            &#34;State file state.json unable to read &#34;
        )</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.validate_configuration_parameters"><code class="name flex">
<span>def <span class="ident">validate_configuration_parameters</span></span>(<span>self, mandatory_params=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Validates config parameters based on provided mandatory parameters.
All provided parameters must be present in config to pass.
ex1.:
par1 = 'par1'
par2 = 'par2'
mandatory_params = [par1, par2]
Validation will fail when one of the above parameters is not found</p>
<p>Two levels of nesting:
Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
=&gt; at least one of the pars has to be present
ex2.
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group1]</p>
<p>Folowing logical expression is evaluated:
Par1 AND Par2 AND (groupPar1 OR groupPar2)</p>
<p>ex3
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group3]</p>
<p>Following logical expression is evaluated:
par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_configuration_parameters(self, mandatory_params=None):
    &#34;&#34;&#34;
            Validates config parameters based on provided mandatory parameters.
            All provided parameters must be present in config to pass.
            ex1.:
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            mandatory_params = [par1, par2]
            Validation will fail when one of the above parameters is not found

            Two levels of nesting:
            Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
            =&gt; at least one of the pars has to be present
            ex2.
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            par3 = &#39;par3&#39;
            groupPar1 = &#39;groupPar1&#39;
            groupPar2 = &#39;groupPar2&#39;
            group1 = [groupPar1, groupPar2]
            group3 = [par3, group1]
            mandatory_params = [par1, par2, group1]

            Folowing logical expression is evaluated:
            Par1 AND Par2 AND (groupPar1 OR groupPar2)

            ex3
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            par3 = &#39;par3&#39;
            groupPar1 = &#39;groupPar1&#39;
            groupPar2 = &#39;groupPar2&#39;
            group1 = [groupPar1, groupPar2]
            group3 = [par3, group1]
            mandatory_params = [par1, par2, group3]

            Following logical expression is evaluated:
            par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
            &#34;&#34;&#34;
    if not mandatory_params:
        mandatory_params = []
    return self._validate_parameters(self.configuration.parameters, mandatory_params, &#39;config parameters&#39;)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.validate_image_parameters"><code class="name flex">
<span>def <span class="ident">validate_image_parameters</span></span>(<span>self, mandatory_params)</span>
</code></dt>
<dd>
<div class="desc"><p>Validates image parameters based on provided mandatory parameters.
All provided parameters must be present in config to pass.
ex1.:
par1 = 'par1'
par2 = 'par2'
mandatory_params = [par1, par2]
Validation will fail when one of the above parameters is not found</p>
<p>Two levels of nesting:
Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
=&gt; at least one of the pars has to be present
ex2.
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group1]</p>
<p>Folowing logical expression is evaluated:
Par1 AND Par2 AND (groupPar1 OR groupPar2)</p>
<p>ex3
par1 = 'par1'
par2 = 'par2'
par3 = 'par3'
groupPar1 = 'groupPar1'
groupPar2 = 'groupPar2'
group1 = [groupPar1, groupPar2]
group3 = [par3, group1]
mandatory_params = [par1, par2, group3]</p>
<p>Following logical expression is evaluated:
par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def validate_image_parameters(self, mandatory_params):
    &#34;&#34;&#34;
            Validates image parameters based on provided mandatory parameters.
            All provided parameters must be present in config to pass.
            ex1.:
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            mandatory_params = [par1, par2]
            Validation will fail when one of the above parameters is not found

            Two levels of nesting:
            Parameters can be grouped as arrays par3 = [groupPar1, groupPar2]
            =&gt; at least one of the pars has to be present
            ex2.
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            par3 = &#39;par3&#39;
            groupPar1 = &#39;groupPar1&#39;
            groupPar2 = &#39;groupPar2&#39;
            group1 = [groupPar1, groupPar2]
            group3 = [par3, group1]
            mandatory_params = [par1, par2, group1]

            Folowing logical expression is evaluated:
            Par1 AND Par2 AND (groupPar1 OR groupPar2)

            ex3
            par1 = &#39;par1&#39;
            par2 = &#39;par2&#39;
            par3 = &#39;par3&#39;
            groupPar1 = &#39;groupPar1&#39;
            groupPar2 = &#39;groupPar2&#39;
            group1 = [groupPar1, groupPar2]
            group3 = [par3, group1]
            mandatory_params = [par1, par2, group3]

            Following logical expression is evaluated:
            par1 AND par2 AND (par3 OR (groupPar1 AND groupPar2))
            &#34;&#34;&#34;
    return self._validate_parameters(self.configuration.image_parameters,
                                     mandatory_params, &#39;image/stack parameters&#39;)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_filedef_manifest"><code class="name flex">
<span>def <span class="ident">write_filedef_manifest</span></span>(<span>self, file_definition:<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Write a table manifest from dao.FileDefinition. Creates the appropriate manifest file in the proper location.</p>
<p>** Usage:**</p>
<pre><code class="language-python">from keboola.component import CommonInterface
from keboola.component import dao

ci = CommonInterface()

# build table definition
file_def = ci.create_out_file_definition(name='my_file.xml', tags=['tag', 'tag2'])
ci.write_filedef_manifest(file_def)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>file_definition</code></strong> :&ensp;<code>dao.FileDefinition</code></dt>
<dd>Initialized dao.FileDefinition object containing manifest.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
def write_filedef_manifest(self, file_definition: dao.FileDefinition):
    &#34;&#34;&#34;
    Write a table manifest from dao.FileDefinition. Creates the appropriate manifest file in the proper location.


    ** Usage:**

    ```python
    from keboola.component import CommonInterface
    from keboola.component import dao

    ci = CommonInterface()

    # build table definition
    file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
    ci.write_filedef_manifest(file_def)
    ```

    Args:
        file_definition (dao.FileDefinition): Initialized dao.FileDefinition object containing manifest.

    Returns:

    &#34;&#34;&#34;
    self.write_manifest(file_definition)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_filedef_manifests"><code class="name flex">
<span>def <span class="ident">write_filedef_manifests</span></span>(<span>self, file_definitions:List[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Process all table definition objects and create appropriate manifest files.</p>
<h2 id="args">Args</h2>
<p>file_definitions:
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
def write_filedef_manifests(self, file_definitions: List[dao.FileDefinition]):
    &#34;&#34;&#34;
    Process all table definition objects and create appropriate manifest files.
    Args:
        file_definitions:

    Returns:

    &#34;&#34;&#34;
    self.write_manifests(file_definitions)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_manifest"><code class="name flex">
<span>def <span class="ident">write_manifest</span></span>(<span>self, io_definition:Union[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>,<a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Write a table manifest from dao.IODefinition. Creates the appropriate manifest file in the proper location.</p>
<p>** Usage:**</p>
<pre><code class="language-python">from keboola.component import CommonInterface
from keboola.component import dao

ci = CommonInterface()

# build table definition
table_def = ci.create_out_table_definition(name='my_new_table', mytable.csv'
                        , incremental = True
                        , table_metadata = tm
                        ))
ci.write_manifest(table_def)

# build file definition
file_def = ci.create_out_file_definition(name='my_file.xml', tags=['tag', 'tag2'])
ci.write_manifest(file_def)
</code></pre>
<h2 id="args">Args</h2>
<p>io_definition Union[dao.FileDefinition, dao.TableDefinition]: Initialized dao.IODefinition
object containing manifest.
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_manifest(self, io_definition: Union[dao.FileDefinition, dao.TableDefinition]):
    &#34;&#34;&#34;
    Write a table manifest from dao.IODefinition. Creates the appropriate manifest file in the proper location.


    ** Usage:**

    ```python
    from keboola.component import CommonInterface
    from keboola.component import dao

    ci = CommonInterface()

    # build table definition
    table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                            , incremental = True
                            , table_metadata = tm
                            ))
    ci.write_manifest(table_def)

    # build file definition
    file_def = ci.create_out_file_definition(name=&#39;my_file.xml&#39;, tags=[&#39;tag&#39;, &#39;tag2&#39;])
    ci.write_manifest(file_def)
    ```

    Args:
        io_definition Union[dao.FileDefinition, dao.TableDefinition]: Initialized dao.IODefinition
         object containing manifest.

    Returns:

    &#34;&#34;&#34;

    manifest = io_definition.get_manifest_dictionary(legacy_queue=self.is_legacy_queue)
    # make dirs if not exist
    os.makedirs(os.path.dirname(io_definition.full_path), exist_ok=True)
    with open(io_definition.full_path + &#39;.manifest&#39;, &#39;w&#39;) as manifest_file:
        json.dump(manifest, manifest_file)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_manifests"><code class="name flex">
<span>def <span class="ident">write_manifests</span></span>(<span>self, io_definitions:List[Union[<a title="keboola.component.dao.FileDefinition" href="dao.html#keboola.component.dao.FileDefinition">FileDefinition</a>,<a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a>]])</span>
</code></dt>
<dd>
<div class="desc"><p>Process all table definition objects and create appropriate manifest files.</p>
<h2 id="args">Args</h2>
<p>io_definitions:
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_manifests(self, io_definitions: List[Union[dao.FileDefinition, dao.TableDefinition]]):
    &#34;&#34;&#34;
    Process all table definition objects and create appropriate manifest files.
    Args:
        io_definitions:

    Returns:

    &#34;&#34;&#34;
    for io_def in io_definitions:
        self.write_manifest(io_def)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_state_file"><code class="name flex">
<span>def <span class="ident">write_state_file</span></span>(<span>self, state_dict:dict)</span>
</code></dt>
<dd>
<div class="desc"><p>Stores <a href="https://developers.keboola.com/extend/common-interface/config-file/#state-file">state file</a>.</p>
<h2 id="args">Args</h2>
<p>state_dict (dict):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def write_state_file(self, state_dict: dict):
    &#34;&#34;&#34;
    Stores [state file](https://developers.keboola.com/extend/common-interface/config-file/#state-file).
    Args:
        state_dict (dict):
    &#34;&#34;&#34;
    if not isinstance(state_dict, dict):
        raise TypeError(&#39;Dictionary expected as a state file datatype!&#39;)

    with open(os.path.join(self.configuration.data_dir, &#39;out&#39;, &#39;state.json&#39;), &#39;w+&#39;) as state_file:
        json.dump(state_dict, state_file)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_tabledef_manifest"><code class="name flex">
<span>def <span class="ident">write_tabledef_manifest</span></span>(<span>self, table_definition:<a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a>)</span>
</code></dt>
<dd>
<div class="desc"><p>Write a table manifest from dao.TableDefinition. Creates the appropriate manifest file in the proper location.</p>
<p>** Usage:**</p>
<pre><code class="language-python">from keboola.component import CommonInterface
from keboola.component import dao

ci = CommonInterface()
tm = dao.TableMetadata()
tm.add_table_description(&quot;My new table&quot;)

# build table definition
table_def = ci.create_out_table_definition(name='my_new_table', mytable.csv'
                        , incremental = True
                        , table_metadata = tm
                        ))
ci.write_tabledef_manifest(table_def)
</code></pre>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>table_definition</code></strong> :&ensp;<code>dao.TableDefinition</code></dt>
<dd>Initialized dao.TableDefinition object containing manifest.</dd>
</dl>
<p>Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifest function&#34;)
def write_tabledef_manifest(self, table_definition: dao.TableDefinition):
    &#34;&#34;&#34;
    Write a table manifest from dao.TableDefinition. Creates the appropriate manifest file in the proper location.


    ** Usage:**

    ```python
    from keboola.component import CommonInterface
    from keboola.component import dao

    ci = CommonInterface()
    tm = dao.TableMetadata()
    tm.add_table_description(&#34;My new table&#34;)

    # build table definition
    table_def = ci.create_out_table_definition(name=&#39;my_new_table&#39;, mytable.csv&#39;
                            , incremental = True
                            , table_metadata = tm
                            ))
    ci.write_tabledef_manifest(table_def)
    ```

    Args:
        table_definition (dao.TableDefinition): Initialized dao.TableDefinition object containing manifest.

    Returns:

    &#34;&#34;&#34;
    self.write_manifest(table_definition)</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.CommonInterface.write_tabledef_manifests"><code class="name flex">
<span>def <span class="ident">write_tabledef_manifests</span></span>(<span>self, table_definitions:List[<a title="keboola.component.dao.TableDefinition" href="dao.html#keboola.component.dao.TableDefinition">TableDefinition</a>])</span>
</code></dt>
<dd>
<div class="desc"><p>Process all table definition objects and create appropriate manifest files.</p>
<h2 id="args">Args</h2>
<p>table_definitions:
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@deprecated(version=&#39;1.3.0&#39;, reason=&#34;You should use write_manifests function&#34;)
def write_tabledef_manifests(self, table_definitions: List[dao.TableDefinition]):
    &#34;&#34;&#34;
    Process all table definition objects and create appropriate manifest files.
    Args:
        table_definitions:

    Returns:

    &#34;&#34;&#34;
    self.write_manifests(table_definitions)</code></pre>
</details>
</dd>
</dl>
</dd>
<dt id="keboola.component.interface.Configuration"><code class="flex name class">
<span>class <span class="ident">Configuration</span></span>
<span>(</span><span>data_folder_path:str)</span>
</code></dt>
<dd>
<div class="desc"><p>Class representing configuration file generated and read
by KBC for docker applications
See docs:
<a href="https://developers.keboola.com/extend/common-interface/config-file/">https://developers.keboola.com/extend/common-interface/config-file/</a></p>
<h2 id="args">Args</h2>
<p>data_folder_path (object):</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class Configuration:
    &#34;&#34;&#34;
    Class representing configuration file generated and read
    by KBC for docker applications
    See docs:
    https://developers.keboola.com/extend/common-interface/config-file/
    &#34;&#34;&#34;

    def __init__(self, data_folder_path: str):
        &#34;&#34;&#34;

        Args:
            data_folder_path (object):
        &#34;&#34;&#34;
        self.config_data = {}
        self.data_dir = data_folder_path

        try:
            with open(os.path.join(data_folder_path, &#39;config.json&#39;), &#39;r&#39;) \
                    as config_file:
                self.config_data = json.load(config_file)
        except (OSError, IOError):
            raise ValueError(
                f&#34;Configuration file config.json not found, verify that the data directory is correct and that the &#34;
                f&#34;config file is present. Dir: &#34;
                f&#34;{self.data_dir}&#34;
            )

        self.parameters = self.config_data.get(&#39;parameters&#39;, {})
        self.image_parameters = self.config_data.get(&#39;image_parameters&#39;, {})
        self.action = self.config_data.get(&#39;action&#39;, &#39;&#39;)
        self.workspace_credentials = self.config_data.get(&#39;authorization&#39;, {}).get(&#39;workspace&#39;, {})

    # ################ PROPERTIES
    @property
    def oauth_credentials(self) -&gt; dao.OauthCredentials:
        &#34;&#34;&#34;
        Returns subscriptable class OauthCredentials

        Returns: OauthCredentials

        &#34;&#34;&#34;
        oauth_credentials = self.config_data.get(&#39;authorization&#39;, {}).get(&#39;oauth_api&#39;, {}).get(&#39;credentials&#39;, {})
        credentials = None
        if oauth_credentials:
            credentials = dao.OauthCredentials(
                id=oauth_credentials.get(&#34;id&#34;, &#39;&#39;),
                created=oauth_credentials.get(&#34;created&#34;, &#39;&#39;),
                data=json.loads(oauth_credentials.get(&#34;#data&#34;, &#39;{}&#39;)),
                oauthVersion=oauth_credentials.get(&#34;oauthVersion&#34;, &#39;&#39;),
                appKey=oauth_credentials.get(&#34;appKey&#34;, &#39;&#39;),
                appSecret=oauth_credentials.get(&#34;#appSecret&#34;, &#39;&#39;)
            )
        return credentials

    @property
    def tables_input_mapping(self) -&gt; List[dao.TableInputMapping]:
        &#34;&#34;&#34;
        List of table [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

        Tables specified in the configuration file.

        Returns: List[TableInputMapping]

        &#34;&#34;&#34;

        tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;input&#39;, {}).get(&#39;tables&#39;, [])
        tables = []
        for table in tables_defs:
            # nested dataclass
            table[&#39;column_types&#39;] = [dao.build_dataclass_from_dict(dao.TableColumnTypes, coltype) for coltype in
                                     table.get(&#39;column_types&#39;, [])]

            im = dao.build_dataclass_from_dict(dao.TableInputMapping, table)
            im.full_path = os.path.normpath(
                os.path.join(
                    self.data_dir,
                    &#39;in&#39;,
                    &#39;tables&#39;,
                    table[&#39;destination&#39;]
                )
            )
            tables.append(im)
        return tables

    @property
    def tables_output_mapping(self) -&gt; List[dao.TableOutputMapping]:
        &#34;&#34;&#34;
        List of table [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

        Get tables which are supposed to be returned when the application finishes. (from configuration[
        &#39;storage&#39;] section.
        Returns: List[TableOutputMapping]

        &#34;&#34;&#34;
        tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;tables&#39;, [])
        tables = []
        for table in tables_defs:
            om = dao.build_dataclass_from_dict(dao.TableOutputMapping, table)
            tables.append(om)
        return tables

    @property
    def files_input_mapping(self) -&gt; List[dao.FileInputMapping]:
        &#34;&#34;&#34;
        List of file [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

        Files specified in the configuration file (defined on component&#39;s input mapping). (from configuration[
        &#39;storage&#39;] section.
        Returns: List[FileInputMapping]

        &#34;&#34;&#34;
        defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
        files = []
        for file in defs:
            om = dao.build_dataclass_from_dict(dao.FileInputMapping, file)
            files.append(om)
        return files

    @property
    def files_output_mapping(self) -&gt; List[dao.FileOutputMapping]:
        &#34;&#34;&#34;
        List of file [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

        Get files which are supposed to be returned when the application finishes. (from configuration[
        &#39;storage&#39;] section.
        Returns:

        &#34;&#34;&#34;
        defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
        files = []
        for file in defs:
            om = dao.build_dataclass_from_dict(dao.FileOutputMapping, file)
            files.append(om)
        return files</code></pre>
</details>
<h3>Instance variables</h3>
<dl>
<dt id="keboola.component.interface.Configuration.files_input_mapping"><code class="name">var <span class="ident">files_input_mapping</span> :List[<a title="keboola.component.dao.FileInputMapping" href="dao.html#keboola.component.dao.FileInputMapping">FileInputMapping</a>]</code></dt>
<dd>
<div class="desc"><p>List of file <a href="https://developers.keboola.com/extend/common-interface/config-file/#files">input mappings</a></p>
<p>Files specified in the configuration file (defined on component's input mapping). (from configuration[
'storage'] section.
Returns: List[FileInputMapping]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def files_input_mapping(self) -&gt; List[dao.FileInputMapping]:
    &#34;&#34;&#34;
    List of file [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

    Files specified in the configuration file (defined on component&#39;s input mapping). (from configuration[
    &#39;storage&#39;] section.
    Returns: List[FileInputMapping]

    &#34;&#34;&#34;
    defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
    files = []
    for file in defs:
        om = dao.build_dataclass_from_dict(dao.FileInputMapping, file)
        files.append(om)
    return files</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.Configuration.files_output_mapping"><code class="name">var <span class="ident">files_output_mapping</span> :List[<a title="keboola.component.dao.FileOutputMapping" href="dao.html#keboola.component.dao.FileOutputMapping">FileOutputMapping</a>]</code></dt>
<dd>
<div class="desc"><p>List of file <a href="https://developers.keboola.com/extend/common-interface/config-file/#files">output mappings</a></p>
<p>Get files which are supposed to be returned when the application finishes. (from configuration[
'storage'] section.
Returns:</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def files_output_mapping(self) -&gt; List[dao.FileOutputMapping]:
    &#34;&#34;&#34;
    List of file [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#files)

    Get files which are supposed to be returned when the application finishes. (from configuration[
    &#39;storage&#39;] section.
    Returns:

    &#34;&#34;&#34;
    defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;files&#39;, [])
    files = []
    for file in defs:
        om = dao.build_dataclass_from_dict(dao.FileOutputMapping, file)
        files.append(om)
    return files</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.Configuration.oauth_credentials"><code class="name">var <span class="ident">oauth_credentials</span> :<a title="keboola.component.dao.OauthCredentials" href="dao.html#keboola.component.dao.OauthCredentials">OauthCredentials</a></code></dt>
<dd>
<div class="desc"><p>Returns subscriptable class OauthCredentials</p>
<p>Returns: OauthCredentials</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def oauth_credentials(self) -&gt; dao.OauthCredentials:
    &#34;&#34;&#34;
    Returns subscriptable class OauthCredentials

    Returns: OauthCredentials

    &#34;&#34;&#34;
    oauth_credentials = self.config_data.get(&#39;authorization&#39;, {}).get(&#39;oauth_api&#39;, {}).get(&#39;credentials&#39;, {})
    credentials = None
    if oauth_credentials:
        credentials = dao.OauthCredentials(
            id=oauth_credentials.get(&#34;id&#34;, &#39;&#39;),
            created=oauth_credentials.get(&#34;created&#34;, &#39;&#39;),
            data=json.loads(oauth_credentials.get(&#34;#data&#34;, &#39;{}&#39;)),
            oauthVersion=oauth_credentials.get(&#34;oauthVersion&#34;, &#39;&#39;),
            appKey=oauth_credentials.get(&#34;appKey&#34;, &#39;&#39;),
            appSecret=oauth_credentials.get(&#34;#appSecret&#34;, &#39;&#39;)
        )
    return credentials</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.Configuration.tables_input_mapping"><code class="name">var <span class="ident">tables_input_mapping</span> :List[<a title="keboola.component.dao.TableInputMapping" href="dao.html#keboola.component.dao.TableInputMapping">TableInputMapping</a>]</code></dt>
<dd>
<div class="desc"><p>List of table <a href="https://developers.keboola.com/extend/common-interface/config-file/#tables">input mappings</a></p>
<p>Tables specified in the configuration file.</p>
<p>Returns: List[TableInputMapping]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tables_input_mapping(self) -&gt; List[dao.TableInputMapping]:
    &#34;&#34;&#34;
    List of table [input mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

    Tables specified in the configuration file.

    Returns: List[TableInputMapping]

    &#34;&#34;&#34;

    tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;input&#39;, {}).get(&#39;tables&#39;, [])
    tables = []
    for table in tables_defs:
        # nested dataclass
        table[&#39;column_types&#39;] = [dao.build_dataclass_from_dict(dao.TableColumnTypes, coltype) for coltype in
                                 table.get(&#39;column_types&#39;, [])]

        im = dao.build_dataclass_from_dict(dao.TableInputMapping, table)
        im.full_path = os.path.normpath(
            os.path.join(
                self.data_dir,
                &#39;in&#39;,
                &#39;tables&#39;,
                table[&#39;destination&#39;]
            )
        )
        tables.append(im)
    return tables</code></pre>
</details>
</dd>
<dt id="keboola.component.interface.Configuration.tables_output_mapping"><code class="name">var <span class="ident">tables_output_mapping</span> :List[<a title="keboola.component.dao.TableOutputMapping" href="dao.html#keboola.component.dao.TableOutputMapping">TableOutputMapping</a>]</code></dt>
<dd>
<div class="desc"><p>List of table <a href="https://developers.keboola.com/extend/common-interface/config-file/#tables">output mappings</a></p>
<p>Get tables which are supposed to be returned when the application finishes. (from configuration[
'storage'] section.
Returns: List[TableOutputMapping]</p></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">@property
def tables_output_mapping(self) -&gt; List[dao.TableOutputMapping]:
    &#34;&#34;&#34;
    List of table [output mappings](https://developers.keboola.com/extend/common-interface/config-file/#tables)

    Get tables which are supposed to be returned when the application finishes. (from configuration[
    &#39;storage&#39;] section.
    Returns: List[TableOutputMapping]

    &#34;&#34;&#34;
    tables_defs = self.config_data.get(&#39;storage&#39;, {}).get(&#39;output&#39;, {}).get(&#39;tables&#39;, [])
    tables = []
    for table in tables_defs:
        om = dao.build_dataclass_from_dict(dao.TableOutputMapping, table)
        tables.append(om)
    return tables</code></pre>
</details>
</dd>
</dl>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="keboola.component" href="index.html">keboola.component</a></code></li>
</ul>
</li>
<li><h3><a href="#header-functions">Functions</a></h3>
<ul class="">
<li><code><a title="keboola.component.interface.init_environment_variables" href="#keboola.component.interface.init_environment_variables">init_environment_variables</a></code></li>
<li><code><a title="keboola.component.interface.register_csv_dialect" href="#keboola.component.interface.register_csv_dialect">register_csv_dialect</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="keboola.component.interface.CommonInterface" href="#keboola.component.interface.CommonInterface">CommonInterface</a></code></h4>
<ul class="">
<li><code><a title="keboola.component.interface.CommonInterface.LOGGING_TYPE_GELF" href="#keboola.component.interface.CommonInterface.LOGGING_TYPE_GELF">LOGGING_TYPE_GELF</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.LOGGING_TYPE_STD" href="#keboola.component.interface.CommonInterface.LOGGING_TYPE_STD">LOGGING_TYPE_STD</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.configuration" href="#keboola.component.interface.CommonInterface.configuration">configuration</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.create_in_table_definition" href="#keboola.component.interface.CommonInterface.create_in_table_definition">create_in_table_definition</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.create_out_file_definition" href="#keboola.component.interface.CommonInterface.create_out_file_definition">create_out_file_definition</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.create_out_table_definition" href="#keboola.component.interface.CommonInterface.create_out_table_definition">create_out_table_definition</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.files_in_path" href="#keboola.component.interface.CommonInterface.files_in_path">files_in_path</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.files_out_path" href="#keboola.component.interface.CommonInterface.files_out_path">files_out_path</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_name" href="#keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_name">get_input_file_definitions_grouped_by_name</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_tag_group" href="#keboola.component.interface.CommonInterface.get_input_file_definitions_grouped_by_tag_group">get_input_file_definitions_grouped_by_tag_group</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_input_files_definitions" href="#keboola.component.interface.CommonInterface.get_input_files_definitions">get_input_files_definitions</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_input_table_definition_by_name" href="#keboola.component.interface.CommonInterface.get_input_table_definition_by_name">get_input_table_definition_by_name</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_input_tables_definitions" href="#keboola.component.interface.CommonInterface.get_input_tables_definitions">get_input_tables_definitions</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.get_state_file" href="#keboola.component.interface.CommonInterface.get_state_file">get_state_file</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.is_legacy_queue" href="#keboola.component.interface.CommonInterface.is_legacy_queue">is_legacy_queue</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.set_default_logger" href="#keboola.component.interface.CommonInterface.set_default_logger">set_default_logger</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.set_gelf_logger" href="#keboola.component.interface.CommonInterface.set_gelf_logger">set_gelf_logger</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.tables_in_path" href="#keboola.component.interface.CommonInterface.tables_in_path">tables_in_path</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.tables_out_path" href="#keboola.component.interface.CommonInterface.tables_out_path">tables_out_path</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.validate_configuration_parameters" href="#keboola.component.interface.CommonInterface.validate_configuration_parameters">validate_configuration_parameters</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.validate_image_parameters" href="#keboola.component.interface.CommonInterface.validate_image_parameters">validate_image_parameters</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_filedef_manifest" href="#keboola.component.interface.CommonInterface.write_filedef_manifest">write_filedef_manifest</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_filedef_manifests" href="#keboola.component.interface.CommonInterface.write_filedef_manifests">write_filedef_manifests</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_manifest" href="#keboola.component.interface.CommonInterface.write_manifest">write_manifest</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_manifests" href="#keboola.component.interface.CommonInterface.write_manifests">write_manifests</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_state_file" href="#keboola.component.interface.CommonInterface.write_state_file">write_state_file</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_tabledef_manifest" href="#keboola.component.interface.CommonInterface.write_tabledef_manifest">write_tabledef_manifest</a></code></li>
<li><code><a title="keboola.component.interface.CommonInterface.write_tabledef_manifests" href="#keboola.component.interface.CommonInterface.write_tabledef_manifests">write_tabledef_manifests</a></code></li>
</ul>
</li>
<li>
<h4><code><a title="keboola.component.interface.Configuration" href="#keboola.component.interface.Configuration">Configuration</a></code></h4>
<ul class="">
<li><code><a title="keboola.component.interface.Configuration.files_input_mapping" href="#keboola.component.interface.Configuration.files_input_mapping">files_input_mapping</a></code></li>
<li><code><a title="keboola.component.interface.Configuration.files_output_mapping" href="#keboola.component.interface.Configuration.files_output_mapping">files_output_mapping</a></code></li>
<li><code><a title="keboola.component.interface.Configuration.oauth_credentials" href="#keboola.component.interface.Configuration.oauth_credentials">oauth_credentials</a></code></li>
<li><code><a title="keboola.component.interface.Configuration.tables_input_mapping" href="#keboola.component.interface.Configuration.tables_input_mapping">tables_input_mapping</a></code></li>
<li><code><a title="keboola.component.interface.Configuration.tables_output_mapping" href="#keboola.component.interface.Configuration.tables_output_mapping">tables_output_mapping</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>